{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d72f73fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Analyzing scan type combinations in: C:\\Users\\User\\github_repos\\AD_CN_all_available_data_final\\ADNI\n",
      "================================================================================\n",
      "üìä Total subjects found: 1540\n",
      "\n",
      "üìã INDIVIDUAL SCAN TYPE COUNTS\n",
      "==================================================\n",
      "MPRAGE                                                       :  527\n",
      "Accelerated_Sagittal_MPRAGE__MSV21_                          :  486\n",
      "Accelerated_Sagittal_MPRAGE                                  :  442\n",
      "MPRAGE_GRAPPA2                                               :  217\n",
      "MPRAGE_Repeat                                                :  210\n",
      "MP-RAGE                                                      :  200\n",
      "MP-RAGE_REPEAT                                               :  165\n",
      "MPRAGE_SENSE2                                                :   95\n",
      "Accelerated_Sagittal_MPRAGE_ND                               :   89\n",
      "Sagittal_3D_Accelerated_MPRAGE                               :   84\n",
      "Accelerated_Sagittal_MPRAGE__MSV22_                          :   64\n",
      "CS_Sagittal_MPRAGE__MSV22_                                   :   40\n",
      "HS_Sagittal_MPRAGE__MSV22_                                   :   33\n",
      "MP-RAGE-Repeat                                               :   33\n",
      "Accelerated_Sagittal_MPRAGE_Phase_A-P                        :   24\n",
      "ADNI_______MPRAGE                                            :   18\n",
      "MPRAGE_SENS                                                  :   17\n",
      "REPEAT_SAG_3D_MPRAGE                                         :   17\n",
      "SAG_3D_MPRAGE                                                :   17\n",
      "SAG_MP-RAGE                                                  :   16\n",
      "MPRAGE_ASO                                                   :   14\n",
      "MPRAGE_SAG                                                   :   14\n",
      "ASO-MPRAGE                                                   :   13\n",
      "MPRAGE_S2_DIS3D                                              :   12\n",
      "MPRAGE_REPEAT                                                :   10\n",
      "Sagittal_3D_Accelerated_MPRAGE__MSV21_                       :   10\n",
      "ADNI-R11___MPRAGE                                            :   10\n",
      "ADNI-R11___MPRAGE-REPEA                                      :   10\n",
      "MPRAGE_ASO_repeat                                            :    9\n",
      "MPRAGE_repeat                                                :    9\n",
      "ADNI_______MPRAGEREPEAT                                      :    8\n",
      "SAG_MP-RAGE_REPEAT                                           :    8\n",
      "MP-RAGE-REPEAT                                               :    7\n",
      "MPRAGE_SENSE                                                 :    7\n",
      "MPRAGE_P2_NO_ANGLE=                                          :    7\n",
      "ASO-MPRAGE_2                                                 :    7\n",
      "ADNI_______MPRAGE_ASO                                        :    7\n",
      "SAG_MPRAGE_GRAPPA2_NO_ANGLE                                  :    7\n",
      "SAG_MPRAGE_NO_ANGLE                                          :    7\n",
      "MPRAGE_3dtfe                                                 :    7\n",
      "MPRAGE_3dtf                                                  :    6\n",
      "MPRAGE_3dtferepeat                                           :    6\n",
      "MPRAGE_repe                                                  :    6\n",
      "MPRAGE_AUTOSHIM_ON                                           :    5\n",
      "MPRAGE_SAGITTAL                                              :    5\n",
      "MPRAGE__NO_ANGLE=                                            :    5\n",
      "MPRAGE__Sag__-_NO_ANGLE=                                     :    5\n",
      "SAG_3D_MPRAGE_NO_ANGLE                                       :    5\n",
      "MPRAGE_NO_ANGLE                                              :    5\n",
      "Sag_MPRAGE                                                   :    5\n",
      "MPRAGE_GRAPPA_2                                              :    4\n",
      "MPRAGE_GRAPPA_2_ND                                           :    4\n",
      "MPRAGE_ND                                                    :    4\n",
      "Sagittal_3D_Accelerated_0_angle_MPRAGE                       :    4\n",
      "ADNI_STUDY_MPRAGE1                                           :    4\n",
      "ADNI_STUDY_MPRAGE2                                           :    4\n",
      "MPRAGEASO                                                    :    4\n",
      "MPRAGEREPEATASO                                              :    4\n",
      "MPRAGE_SENSE_repeat                                          :    4\n",
      "ADNI_______MPRAGEadni2                                       :    3\n",
      "MP-RAGE__REPEAT                                              :    3\n",
      "IR-FSPGR__replaces_MP-Rage_                                  :    3\n",
      "ADNI_SH____MPRAGE_ASO                                        :    3\n",
      "ADNI_SH____MPRAGE_ASOX2                                      :    3\n",
      "Accelerated_Sagittal_MPRAGE_MPR_Cor                          :    3\n",
      "Accelerated_Sagittal_MPRAGE_MPR_Tra                          :    3\n",
      "ADNI_new___MPRAGE                                            :    3\n",
      "ADNI_new___MPRAGErepeat                                      :    3\n",
      "MPRAGEadni                                                   :    2\n",
      "ADNI_______MPRAGE-1.2_                                       :    2\n",
      "___________MPRAGE                                            :    2\n",
      "ADNI_______MPRAGE_#2                                         :    2\n",
      "MPRAGE_P2_NO_ANGLE                                           :    2\n",
      "MPRAGE__NO_ANGLE                                             :    2\n",
      "MP-RAGE_repeat                                               :    2\n",
      "MPRAGE_GRAPPA2_S3_DIS3D                                      :    2\n",
      "ADNI-R11-ASASO-MPRAGE                                        :    2\n",
      "ADNI-R11-ASASO-MPRAGE_2                                      :    2\n",
      "MP-RAGE_Repeat                                               :    2\n",
      "ADNI_______MPRAGE_REPET                                      :    2\n",
      "MPRAGEREPEAT                                                 :    2\n",
      "MPRAGE_REPE                                                  :    1\n",
      "MP-RAGE_24_FOV                                               :    1\n",
      "MP-RAGE_24_FOV_REPEAT                                        :    1\n",
      "Repeat_Accelerated_Sagittal_MPRAGE                           :    1\n",
      "REPEAT_MP-RAGE                                               :    1\n",
      "ADNI_______MPRAGEadni2B                                      :    1\n",
      "ADNI_______MPRAGEadni22                                      :    1\n",
      "MPRAGE__REPEAT                                               :    1\n",
      "Accelerated_Sagittal_MPRAGE_REPEAT                           :    1\n",
      "MPRAGE_Repeat_2                                              :    1\n",
      "MP-RAGE_REPEAT__SERIES_3_                                    :    1\n",
      "MP-RAGE__SERIES_2_                                           :    1\n",
      "CS_Sagittal_3D_Accelerated_MPRAGE                            :    1\n",
      "MPRAGE_Repeat_rpt                                            :    1\n",
      "MP-RAGE_#2                                                   :    1\n",
      "ADNI3_Accelerated_MPRAGE                                     :    1\n",
      "IGIV_STUDY_MPRAGE_ASO_S                                      :    1\n",
      "ADNI_______MPRAGE_2ND                                        :    1\n",
      "MPRAGE_2ND                                                   :    1\n",
      "MPRAGE_REPEAT_ASO                                            :    1\n",
      "MP-RAGE_-_REPEAT                                             :    1\n",
      "MPRAGE__Repeat                                               :    1\n",
      "ASO-MPRAGE__2_                                               :    1\n",
      "MPRAGE_SENSE2_SENSE                                          :    1\n",
      "MPRAGE_S2_DIS2D                                              :    1\n",
      "MP-RAGE_REPEAT_#3                                            :    1\n",
      "SAG_3D_MPRAGE,_no_angle                                      :    1\n",
      "Sagittal_3D_Accelerated_MPRAGE_REPEAT                        :    1\n",
      "___________MPRAGE_3dtf                                       :    1\n",
      "___________MPRAGE_repe                                       :    1\n",
      "Accelerated_Sagittal_MPRAGE_MSV21                            :    1\n",
      "VWIP_Coronal_3D_Accelerated_MPRAGE                           :    1\n",
      "3D_MPRAGE                                                    :    1\n",
      "\n",
      "üîó UNIQUE SCAN TYPE COMBINATIONS\n",
      "================================================================================\n",
      "Total unique combinations: 181\n",
      "\n",
      "üìä Combination Counts (sorted by frequency):\n",
      "--------------------------------------------------------------------------------\n",
      "  1. Accelerated_Sagittal_MPRAGE__MSV21_                                    :  301 subjects\n",
      "  2. Accelerated_Sagittal_MPRAGE                                            :  184 subjects\n",
      "  3. MPRAGE + MPRAGE_Repeat                                                 :  126 subjects\n",
      "  4. MPRAGE + MPRAGE_GRAPPA2                                                :  119 subjects\n",
      "  5. MP-RAGE + MP-RAGE_REPEAT                                               :  104 subjects\n",
      "  6. Accelerated_Sagittal_MPRAGE + Accelerated_Sagittal_MPRAGE__MSV21_      :   88 subjects\n",
      "  7. MPRAGE + MPRAGE_SENSE2                                                 :   60 subjects\n",
      "  8. Accelerated_Sagittal_MPRAGE + Accelerated_Sagittal_MPRAGE_ND           :   54 subjects\n",
      "  9. Sagittal_3D_Accelerated_MPRAGE                                         :   42 subjects\n",
      " 10. Accelerated_Sagittal_MPRAGE__MSV22_ + HS_Sagittal_MPRAGE__MSV22_       :   33 subjects\n",
      " 11. Accelerated_Sagittal_MPRAGE__MSV22_                                    :   29 subjects\n",
      " 12. Accelerated_Sagittal_MPRAGE__MSV21_ + CS_Sagittal_MPRAGE__MSV22_       :   25 subjects\n",
      " 13. Accelerated_Sagittal_MPRAGE + MPRAGE + MPRAGE_GRAPPA2                  :   20 subjects\n",
      " 14. Accelerated_Sagittal_MPRAGE + Accelerated_Sagittal_MPRAGE_ND + Accelerated_Sagittal_MPRAGE__MSV21_ :   20 subjects\n",
      " 15. MP-RAGE + MP-RAGE_REPEAT + MPRAGE + MPRAGE_Repeat                      :   11 subjects\n",
      " 16. Accelerated_Sagittal_MPRAGE_Phase_A-P                                  :   11 subjects\n",
      " 17. Accelerated_Sagittal_MPRAGE + Accelerated_Sagittal_MPRAGE__MSV21_ + MPRAGE + MPRAGE_GRAPPA2 :   10 subjects\n",
      " 18. MPRAGE + MPRAGE_SENSE2 + Sagittal_3D_Accelerated_MPRAGE                :    9 subjects\n",
      " 19. Sagittal_3D_Accelerated_MPRAGE__MSV21_                                 :    9 subjects\n",
      " 20. MPRAGE + MPRAGE_GRAPPA2 + MPRAGE_Repeat                                :    8 subjects\n",
      " 21. MPRAGE + MPRAGE_Repeat + MPRAGE_SAG                                    :    7 subjects\n",
      " 22. MP-RAGE + MP-RAGE-Repeat                                               :    7 subjects\n",
      " 23. MPRAGE + MPRAGE_GRAPPA2 + MPRAGE_S2_DIS3D                              :    6 subjects\n",
      " 24. MP-RAGE + MP-RAGE-Repeat + MPRAGE + MPRAGE_Repeat                      :    6 subjects\n",
      " 25. MPRAGE + MPRAGE_GRAPPA2 + SAG_MPRAGE_GRAPPA2_NO_ANGLE + SAG_MPRAGE_NO_ANGLE :    6 subjects\n",
      " 26. Accelerated_Sagittal_MPRAGE__MSV21_ + CS_Sagittal_MPRAGE__MSV22_ + Sagittal_3D_Accelerated_MPRAGE :    5 subjects\n",
      " 27. Accelerated_Sagittal_MPRAGE + Accelerated_Sagittal_MPRAGE_ND + Accelerated_Sagittal_MPRAGE__MSV21_ + MPRAGE + MPRAGE_GRAPPA2 :    5 subjects\n",
      " 28. MPRAGE + MPRAGE_Repeat + MPRAGE_SAG + MPRAGE_SAGITTAL                  :    5 subjects\n",
      " 29. Accelerated_Sagittal_MPRAGE + Accelerated_Sagittal_MPRAGE_ND + MPRAGE + MPRAGE_GRAPPA2 :    5 subjects\n",
      " 30. CS_Sagittal_MPRAGE__MSV22_                                             :    5 subjects\n",
      " 31. Accelerated_Sagittal_MPRAGE__MSV21_ + Sagittal_3D_Accelerated_MPRAGE   :    5 subjects\n",
      " 32. MP-RAGE + MP-RAGE_REPEAT + MPRAGE + MPRAGE_GRAPPA2                     :    4 subjects\n",
      " 33. Accelerated_Sagittal_MPRAGE + MPRAGE + MPRAGE_GRAPPA2 + MPRAGE_Repeat  :    4 subjects\n",
      " 34. Accelerated_Sagittal_MPRAGE_Phase_A-P + Accelerated_Sagittal_MPRAGE__MSV21_ :    4 subjects\n",
      " 35. MP-RAGE + MP-RAGE_REPEAT + SAG_MP-RAGE                                 :    4 subjects\n",
      " 36. MP-RAGE + MP-RAGE-Repeat + MPRAGE + MPRAGE_Repeat + REPEAT_SAG_3D_MPRAGE + SAG_3D_MPRAGE :    4 subjects\n",
      " 37. MP-RAGE + MP-RAGE_REPEAT + MPRAGE + MPRAGE_Repeat + Sag_MPRAGE         :    4 subjects\n",
      " 38. MP-RAGE + MP-RAGE_REPEAT + MPRAGE_3dtf + MPRAGE_3dtfe + MPRAGE_3dtferepeat + MPRAGE_repe + MPRAGE_repeat :    4 subjects\n",
      " 39. MP-RAGE + MP-RAGE_REPEAT + MPRAGE + MPRAGE_SENSE2                      :    3 subjects\n",
      " 40. MP-RAGE + MP-RAGE-REPEAT + MP-RAGE_REPEAT                              :    3 subjects\n",
      " 41. MPRAGE + MPRAGE_GRAPPA_2 + MPRAGE_GRAPPA_2_ND + MPRAGE_ND + MPRAGE_SENSE2 :    3 subjects\n",
      " 42. Accelerated_Sagittal_MPRAGE + Accelerated_Sagittal_MPRAGE_Phase_A-P + Accelerated_Sagittal_MPRAGE__MSV21_ :    3 subjects\n",
      " 43. Accelerated_Sagittal_MPRAGE + Accelerated_Sagittal_MPRAGE_Phase_A-P    :    3 subjects\n",
      " 44. MP-RAGE + MP-RAGE-Repeat + MPRAGE_NO_ANGLE + MPRAGE_Repeat + REPEAT_SAG_3D_MPRAGE + SAG_3D_MPRAGE :    3 subjects\n",
      " 45. Accelerated_Sagittal_MPRAGE + Sagittal_3D_Accelerated_MPRAGE           :    3 subjects\n",
      " 46. Accelerated_Sagittal_MPRAGE + Accelerated_Sagittal_MPRAGE__MSV21_ + MPRAGE + MPRAGE_SENSE2 :    2 subjects\n",
      " 47. Accelerated_Sagittal_MPRAGE + MPRAGE + MPRAGE_SENSE2                   :    2 subjects\n",
      " 48. ASO-MPRAGE + MPRAGE                                                    :    2 subjects\n",
      " 49. MPRAGE + MPRAGE_SENS                                                   :    2 subjects\n",
      " 50. MPRAGE + MPRAGE_AUTOSHIM_ON                                            :    2 subjects\n",
      " 51. MPRAGE + MPRAGE_REPEAT                                                 :    2 subjects\n",
      " 52. MPRAGE + MPRAGE_ASO + MPRAGE_ASO_repeat + MPRAGE_Repeat + MPRAGE_SENS  :    2 subjects\n",
      " 53. MPRAGE + MPRAGE_SENSE2 + Sagittal_3D_Accelerated_0_angle_MPRAGE + Sagittal_3D_Accelerated_MPRAGE :    2 subjects\n",
      " 54. Sagittal_3D_Accelerated_0_angle_MPRAGE                                 :    2 subjects\n",
      " 55. Accelerated_Sagittal_MPRAGE + MPRAGE + MPRAGE_Repeat                   :    2 subjects\n",
      " 56. Accelerated_Sagittal_MPRAGE + MP-RAGE + MP-RAGE_REPEAT + MPRAGE + MPRAGE_Repeat :    2 subjects\n",
      " 57. Accelerated_Sagittal_MPRAGE + MPRAGE + MPRAGE_GRAPPA2 + MPRAGE_P2_NO_ANGLE + MPRAGE_P2_NO_ANGLE= + MPRAGE__NO_ANGLE + MPRAGE__NO_ANGLE= + MPRAGE__Sag__-_NO_ANGLE= :    2 subjects\n",
      " 58. MPRAGE + MPRAGE_GRAPPA2 + MPRAGE_P2_NO_ANGLE= + MPRAGE__NO_ANGLE=      :    2 subjects\n",
      " 59. MPRAGE + MPRAGE_GRAPPA2 + MPRAGE_P2_NO_ANGLE= + MPRAGE__Sag__-_NO_ANGLE= :    2 subjects\n",
      " 60. Accelerated_Sagittal_MPRAGE + Accelerated_Sagittal_MPRAGE_Phase_A-P + Accelerated_Sagittal_MPRAGE__MSV21_ + MPRAGE + MPRAGE_GRAPPA2 :    2 subjects\n",
      " 61. ADNI_______MPRAGE                                                      :    2 subjects\n",
      " 62. ADNI_SH____MPRAGE_ASO + ADNI_SH____MPRAGE_ASOX2 + ADNI_______MPRAGE + Accelerated_Sagittal_MPRAGE + MPRAGE + MPRAGE_GRAPPA2 :    2 subjects\n",
      " 63. Accelerated_Sagittal_MPRAGE + MPRAGE + MPRAGE_GRAPPA2 + MPRAGE_GRAPPA2_S3_DIS3D + MPRAGE_S2_DIS3D :    2 subjects\n",
      " 64. ADNI-R11___MPRAGE + ADNI-R11___MPRAGE-REPEA + ADNI_STUDY_MPRAGE1 + ADNI_STUDY_MPRAGE2 + ADNI_new___MPRAGE + ADNI_new___MPRAGErepeat + ASO-MPRAGE + ASO-MPRAGE_2 :    2 subjects\n",
      " 65. ADNI-R11___MPRAGE + ADNI-R11___MPRAGE-REPEA                            :    2 subjects\n",
      " 66. Accelerated_Sagittal_MPRAGE__MSV21_ + MPRAGE + MPRAGE_SENSE2 + Sagittal_3D_Accelerated_MPRAGE :    2 subjects\n",
      " 67. ADNI_______MPRAGE + ADNI_______MPRAGEREPEAT + ADNI_______MPRAGE_ASO + ADNI_______MPRAGE_REPET :    2 subjects\n",
      " 68. MP-RAGE + MP-RAGE-Repeat + MPRAGE + MPRAGE_Repeat + REPEAT_SAG_3D_MPRAGE + SAG_3D_MPRAGE + SAG_3D_MPRAGE_NO_ANGLE :    2 subjects\n",
      " 69. MP-RAGE + MP-RAGE-Repeat + MPRAGE_NO_ANGLE + MPRAGE_Repeat + REPEAT_SAG_3D_MPRAGE + SAG_3D_MPRAGE + SAG_3D_MPRAGE_NO_ANGLE :    2 subjects\n",
      " 70. REPEAT_SAG_3D_MPRAGE + SAG_3D_MPRAGE                                   :    2 subjects\n",
      " 71. MP-RAGE + MP-RAGE_REPEAT + SAG_MP-RAGE + SAG_MP-RAGE_REPEAT            :    2 subjects\n",
      " 72. MPRAGE_SENSE + MPRAGE_SENSE_repeat + SAG_MP-RAGE + SAG_MP-RAGE_REPEAT  :    2 subjects\n",
      " 73. SAG_MP-RAGE + SAG_MP-RAGE_REPEAT                                       :    2 subjects\n",
      " 74. Accelerated_Sagittal_MPRAGE__MSV22_ + MPRAGE + MPRAGE_GRAPPA2          :    2 subjects\n",
      " 75. Accelerated_Sagittal_MPRAGE + Accelerated_Sagittal_MPRAGE_MPR_Cor + Accelerated_Sagittal_MPRAGE_MPR_Tra :    2 subjects\n",
      " 76. Accelerated_Sagittal_MPRAGE + Accelerated_Sagittal_MPRAGE__MSV21_ + MP-RAGE + MP-RAGE_REPEAT + MPRAGE + MPRAGE_REPE + MPRAGE_REPEAT + MPRAGE_SENS + MPRAGE_SENSE2 :    1 subjects\n",
      " 77. MP-RAGE + MP-RAGE_REPEAT + MPRAGE + MPRAGE_REPEAT + MPRAGE_SENS        :    1 subjects\n",
      " 78. MP-RAGE + MP-RAGE_24_FOV + MP-RAGE_24_FOV_REPEAT + MP-RAGE_REPEAT + MPRAGE + MPRAGE_REPEAT :    1 subjects\n",
      " 79. MP-RAGE + MP-RAGE_REPEAT + MPRAGE + MPRAGE_REPEAT                      :    1 subjects\n",
      " 80. Accelerated_Sagittal_MPRAGE + MP-RAGE + MP-RAGE_REPEAT + MPRAGE + MPRAGE_REPEAT + MPRAGE_SENSE2 :    1 subjects\n",
      " 81. Accelerated_Sagittal_MPRAGE + Accelerated_Sagittal_MPRAGE__MSV21_ + MP-RAGE + MP-RAGE_REPEAT + MPRAGE + MPRAGE_REPEAT + MPRAGE_SENSE2 + Repeat_Accelerated_Sagittal_MPRAGE :    1 subjects\n",
      " 82. MP-RAGE + MP-RAGE_REPEAT + MPRAGE + MPRAGE_SENSE2 + Sagittal_3D_Accelerated_MPRAGE :    1 subjects\n",
      " 83. MP-RAGE + MP-RAGE_REPEAT + MPRAGE + MPRAGE_SENSE2 + REPEAT_MP-RAGE + Sagittal_3D_Accelerated_MPRAGE :    1 subjects\n",
      " 84. Sagittal_3D_Accelerated_MPRAGE + Sagittal_3D_Accelerated_MPRAGE__MSV21_ :    1 subjects\n",
      " 85. Accelerated_Sagittal_MPRAGE + Accelerated_Sagittal_MPRAGE__MSV21_ + MP-RAGE + MP-RAGE_REPEAT :    1 subjects\n",
      " 86. ADNI_______MPRAGE + ASO-MPRAGE + MPRAGE + MPRAGE_SENS                  :    1 subjects\n",
      " 87. ADNI_______MPRAGE + ASO-MPRAGE + MPRAGE                                :    1 subjects\n",
      " 88. ASO-MPRAGE + MPRAGE + MPRAGE_SENS                                      :    1 subjects\n",
      " 89. ASO-MPRAGE + MPRAGE_SENS                                               :    1 subjects\n",
      " 90. Accelerated_Sagittal_MPRAGE + Accelerated_Sagittal_MPRAGE__MSV21_ + MPRAGE + MPRAGE_GRAPPA2 + MPRAGE_Repeat :    1 subjects\n",
      " 91. MPRAGE + MPRAGE_AUTOSHIM_ON + MPRAGE_SENS                              :    1 subjects\n",
      " 92. ADNI_______MPRAGEadni2 + MPRAGE + MPRAGE_AUTOSHIM_ON + MPRAGE_SENS + MPRAGEadni :    1 subjects\n",
      " 93. ADNI_______MPRAGEadni2 + ADNI_______MPRAGEadni2B + MPRAGE + MPRAGE_AUTOSHIM_ON + MPRAGEadni :    1 subjects\n",
      " 94. ADNI_______MPRAGEadni2 + ADNI_______MPRAGEadni22 + MPRAGE              :    1 subjects\n",
      " 95. MPRAGE + MPRAGE_REPEAT + MPRAGE__REPEAT                                :    1 subjects\n",
      " 96. Accelerated_Sagittal_MPRAGE + Accelerated_Sagittal_MPRAGE_ND + MPRAGE + MPRAGE_GRAPPA_2 + MPRAGE_GRAPPA_2_ND + MPRAGE_ND + MPRAGE_SENSE2 :    1 subjects\n",
      " 97. Accelerated_Sagittal_MPRAGE + Accelerated_Sagittal_MPRAGE_REPEAT       :    1 subjects\n",
      " 98. ADNI_______MPRAGE-1.2_ + MPRAGE + MPRAGE_ASO_repeat + MPRAGE_Repeat    :    1 subjects\n",
      " 99. ADNI_______MPRAGE-1.2_ + MPRAGE + MPRAGE_Repeat                        :    1 subjects\n",
      "100. MPRAGE_SENS                                                            :    1 subjects\n",
      "101. MPRAGE + MPRAGE_ASO + MPRAGE_ASO_repeat + MPRAGE_SENS                  :    1 subjects\n",
      "102. MPRAGE + MPRAGE_ASO + MPRAGE_ASO_repeat + MPRAGE_SENS + MPRAGE_SENSE + ___________MPRAGE :    1 subjects\n",
      "103. MPRAGE + MPRAGE_ASO_repeat + MPRAGE_SENS + MPRAGE_SENSE                :    1 subjects\n",
      "104. MPRAGE + MPRAGE_ASO + MPRAGE_ASO_repeat + MPRAGE_Repeat + MPRAGE_Repeat_2 + MPRAGE_SENS :    1 subjects\n",
      "105. Accelerated_Sagittal_MPRAGE__MSV21_ + CS_Sagittal_MPRAGE__MSV22_ + MPRAGE + MPRAGE_SENSE2 + Sagittal_3D_Accelerated_MPRAGE :    1 subjects\n",
      "106. Accelerated_Sagittal_MPRAGE + Accelerated_Sagittal_MPRAGE_ND + Sagittal_3D_Accelerated_MPRAGE :    1 subjects\n",
      "107. MPRAGE + MPRAGE_GRAPPA2 + MPRAGE_repeat                                :    1 subjects\n",
      "108. MP-RAGE + MP-RAGE_REPEAT + MP-RAGE_REPEAT__SERIES_3_ + MP-RAGE__SERIES_2_ :    1 subjects\n",
      "109. MP-RAGE + MP-RAGE-Repeat + MPRAGE_ASO + MPRAGE_ASO_repeat + Sagittal_3D_Accelerated_MPRAGE :    1 subjects\n",
      "110. CS_Sagittal_3D_Accelerated_MPRAGE + Sagittal_3D_Accelerated_MPRAGE     :    1 subjects\n",
      "111. MP-RAGE + MP-RAGE-Repeat + MPRAGE + MPRAGE_ASO + MPRAGE_ASO_repeat + MPRAGE_Repeat :    1 subjects\n",
      "112. ADNI_______MPRAGE + ADNI_______MPRAGE_#2 + MPRAGE                      :    1 subjects\n",
      "113. ADNI_______MPRAGE + ADNI_______MPRAGE_#2 + MPRAGE + MPRAGE_ASO         :    1 subjects\n",
      "114. MPRAGE + MPRAGE_ASO                                                    :    1 subjects\n",
      "115. MPRAGE + MPRAGE_ASO + MPRAGE_Repeat + ___________MPRAGE                :    1 subjects\n",
      "116. Accelerated_Sagittal_MPRAGE + MPRAGE + MPRAGE_ASO + MPRAGE_GRAPPA2 + MPRAGE_Repeat :    1 subjects\n",
      "117. MPRAGE                                                                 :    1 subjects\n",
      "118. MPRAGE + MPRAGE_ASO + MPRAGE_Repeat                                    :    1 subjects\n",
      "119. Accelerated_Sagittal_MPRAGE + MPRAGE + MPRAGE_ASO + MPRAGE_Repeat      :    1 subjects\n",
      "120. Accelerated_Sagittal_MPRAGE + MP-RAGE + MP-RAGE_REPEAT                 :    1 subjects\n",
      "121. Accelerated_Sagittal_MPRAGE + MPRAGE + MPRAGE_GRAPPA2 + MPRAGE_P2_NO_ANGLE= + MPRAGE__NO_ANGLE= + MPRAGE__Sag__-_NO_ANGLE= :    1 subjects\n",
      "122. Accelerated_Sagittal_MPRAGE + Accelerated_Sagittal_MPRAGE_Phase_A-P + MPRAGE + MPRAGE_GRAPPA2 + MPRAGE_Repeat + MPRAGE_Repeat_rpt :    1 subjects\n",
      "123. Accelerated_Sagittal_MPRAGE + MP-RAGE + MP-RAGE-Repeat + MP-RAGE_REPEAT + MPRAGE + MPRAGE_GRAPPA2 + MPRAGE_REPEAT :    1 subjects\n",
      "124. MP-RAGE + MP-RAGE-REPEAT                                               :    1 subjects\n",
      "125. Accelerated_Sagittal_MPRAGE + MP-RAGE + MP-RAGE-Repeat + MP-RAGE_REPEAT + MPRAGE + MPRAGE_GRAPPA2 :    1 subjects\n",
      "126. MP-RAGE + MP-RAGE-Repeat + MP-RAGE__REPEAT + MPRAGE + MPRAGE_GRAPPA2   :    1 subjects\n",
      "127. MP-RAGE + MP-RAGE_REPEAT + MP-RAGE__REPEAT                             :    1 subjects\n",
      "128. MP-RAGE + MP-RAGE_#2 + MP-RAGE_repeat                                  :    1 subjects\n",
      "129. IR-FSPGR__replaces_MP-Rage_ + MPRAGE + MPRAGE_Repeat                   :    1 subjects\n",
      "130. ADNI3_Accelerated_MPRAGE                                               :    1 subjects\n",
      "131. ADNI_SH____MPRAGE_ASO + ADNI_SH____MPRAGE_ASOX2 + ADNI_______MPRAGE + Accelerated_Sagittal_MPRAGE + Accelerated_Sagittal_MPRAGE__MSV21_ + IGIV_STUDY_MPRAGE_ASO_S :    1 subjects\n",
      "132. Accelerated_Sagittal_MPRAGE + Accelerated_Sagittal_MPRAGE_MPR_Cor + Accelerated_Sagittal_MPRAGE_MPR_Tra + Accelerated_Sagittal_MPRAGE__MSV21_ + MPRAGE + MPRAGE_GRAPPA2 + MPRAGE_SAG :    1 subjects\n",
      "133. ADNI_______MPRAGE + ADNI_______MPRAGE_2ND + MPRAGE + MPRAGE_2ND + MPRAGE_ASO + MPRAGE_REPEAT_ASO + MPRAGE_SENS :    1 subjects\n",
      "134. Accelerated_Sagittal_MPRAGE__MSV21_ + MPRAGE + MPRAGE_GRAPPA2 + MPRAGE_Repeat + MPRAGE_S2_DIS3D :    1 subjects\n",
      "135. MPRAGE + MPRAGE_GRAPPA2 + MPRAGE_Repeat + MPRAGE_S2_DIS3D              :    1 subjects\n",
      "136. Accelerated_Sagittal_MPRAGE + MPRAGE + MPRAGE_GRAPPA2 + MPRAGE_S2_DIS3D :    1 subjects\n",
      "137. Accelerated_Sagittal_MPRAGE__MSV21_ + MPRAGE + MPRAGE_GRAPPA2 + MPRAGE_S2_DIS3D :    1 subjects\n",
      "138. MPRAGE_Repeat + MPRAGE_SAG                                             :    1 subjects\n",
      "139. MP-RAGE + MP-RAGE_-_REPEAT + MP-RAGE_REPEAT                            :    1 subjects\n",
      "140. MP-RAGE + MP-RAGE_repeat                                               :    1 subjects\n",
      "141. MPRAGE + MPRAGE_Repeat + MPRAGE__Repeat                                :    1 subjects\n",
      "142. ADNI-R11___MPRAGE + ADNI-R11___MPRAGE-REPEA + ADNI_STUDY_MPRAGE1 + ADNI_STUDY_MPRAGE2 + ASO-MPRAGE + ASO-MPRAGE_2 :    1 subjects\n",
      "143. ADNI-R11___MPRAGE + ADNI-R11___MPRAGE-REPEA + ADNI_STUDY_MPRAGE1 + ADNI_STUDY_MPRAGE2 + ASO-MPRAGE + ASO-MPRAGE_2 + Accelerated_Sagittal_MPRAGE__MSV21_ + CS_Sagittal_MPRAGE__MSV22_ + MPRAGE + MPRAGE_SENSE2 + Sagittal_3D_Accelerated_MPRAGE :    1 subjects\n",
      "144. ADNI-R11___MPRAGE + ADNI-R11___MPRAGE-REPEA + ASO-MPRAGE + ASO-MPRAGE_2 :    1 subjects\n",
      "145. ADNI-R11-ASASO-MPRAGE + ADNI-R11-ASASO-MPRAGE_2 + ADNI-R11___MPRAGE + ADNI-R11___MPRAGE-REPEA + ASO-MPRAGE + ASO-MPRAGE_2 :    1 subjects\n",
      "146. ADNI-R11-ASASO-MPRAGE + ADNI-R11-ASASO-MPRAGE_2 + ADNI-R11___MPRAGE + ADNI-R11___MPRAGE-REPEA :    1 subjects\n",
      "147. ADNI-R11___MPRAGE + ADNI-R11___MPRAGE-REPEA + ADNI_new___MPRAGE + ADNI_new___MPRAGErepeat + ASO-MPRAGE + ASO-MPRAGE_2 + ASO-MPRAGE__2_ + MPRAGE + MPRAGE_SENSE + MPRAGE_SENSE2 + MPRAGE_SENSE2_SENSE + Sagittal_3D_Accelerated_MPRAGE :    1 subjects\n",
      "148. Accelerated_Sagittal_MPRAGE__MSV21_ + CS_Sagittal_MPRAGE__MSV22_ + MPRAGE + MPRAGE_SENSE2 :    1 subjects\n",
      "149. CS_Sagittal_MPRAGE__MSV22_ + Sagittal_3D_Accelerated_MPRAGE            :    1 subjects\n",
      "150. Accelerated_Sagittal_MPRAGE + MPRAGE + MPRAGE_GRAPPA2 + MPRAGE_Repeat + MPRAGE_S2_DIS2D :    1 subjects\n",
      "151. MP-RAGE + MP-RAGE_Repeat                                               :    1 subjects\n",
      "152. Accelerated_Sagittal_MPRAGE + Accelerated_Sagittal_MPRAGE_ND + MP-RAGE + MP-RAGE-Repeat + MPRAGE + MPRAGE_Repeat :    1 subjects\n",
      "153. MPRAGE + MPRAGE_repeat                                                 :    1 subjects\n",
      "154. ADNI_______MPRAGE + ADNI_______MPRAGEREPEAT + ADNI_______MPRAGE_ASO + Accelerated_Sagittal_MPRAGE__MSV21_ + MPRAGEASO + MPRAGEREPEATASO + Sagittal_3D_Accelerated_MPRAGE :    1 subjects\n",
      "155. ADNI_______MPRAGE + ADNI_______MPRAGEREPEAT                            :    1 subjects\n",
      "156. ADNI_______MPRAGE + ADNI_______MPRAGEREPEAT + ADNI_______MPRAGE_ASO + MPRAGE + MPRAGEASO + MPRAGEREPEAT + MPRAGEREPEATASO + Sagittal_3D_Accelerated_MPRAGE :    1 subjects\n",
      "157. ADNI_______MPRAGE + ADNI_______MPRAGEREPEAT + ADNI_______MPRAGE_ASO + MPRAGE + MPRAGEASO + MPRAGEREPEAT + MPRAGEREPEATASO :    1 subjects\n",
      "158. ADNI_______MPRAGE + ADNI_______MPRAGEREPEAT + ADNI_______MPRAGE_ASO    :    1 subjects\n",
      "159. ADNI_______MPRAGE + ADNI_______MPRAGEREPEAT + ADNI_______MPRAGE_ASO + MPRAGEASO + MPRAGEREPEATASO + Sagittal_3D_Accelerated_MPRAGE :    1 subjects\n",
      "160. MP-RAGE + MP-RAGE-REPEAT + MP-RAGE_REPEAT + MP-RAGE_REPEAT_#3 + SAG_MP-RAGE :    1 subjects\n",
      "161. MP-RAGE_REPEAT + SAG_MP-RAGE                                           :    1 subjects\n",
      "162. IR-FSPGR__replaces_MP-Rage_ + MP-RAGE + MP-RAGE-REPEAT + MP-RAGE_REPEAT + SAG_MP-RAGE :    1 subjects\n",
      "163. IR-FSPGR__replaces_MP-Rage_ + MP-RAGE + MP-RAGE_REPEAT + SAG_MP-RAGE   :    1 subjects\n",
      "164. MP-RAGE + MP-RAGE-Repeat + MPRAGE + MPRAGE_Repeat + REPEAT_SAG_3D_MPRAGE + SAG_3D_MPRAGE + SAG_3D_MPRAGE,_no_angle :    1 subjects\n",
      "165. MP-RAGE + MP-RAGE-Repeat + REPEAT_SAG_3D_MPRAGE + SAG_3D_MPRAGE        :    1 subjects\n",
      "166. MPRAGE + MPRAGE_Repeat + REPEAT_SAG_3D_MPRAGE + SAG_3D_MPRAGE          :    1 subjects\n",
      "167. MPRAGE + MPRAGE_Repeat + REPEAT_SAG_3D_MPRAGE + SAG_3D_MPRAGE + SAG_3D_MPRAGE_NO_ANGLE :    1 subjects\n",
      "168. Accelerated_Sagittal_MPRAGE + MPRAGE + MPRAGE_GRAPPA2 + SAG_MPRAGE_GRAPPA2_NO_ANGLE + SAG_MPRAGE_NO_ANGLE :    1 subjects\n",
      "169. MP-RAGE + MP-RAGE-Repeat + MPRAGE + MPRAGE_SENSE2                      :    1 subjects\n",
      "170. MP-RAGE + MP-RAGE_REPEAT + MPRAGE_SENSE + MPRAGE_SENSE_repeat + SAG_MP-RAGE + SAG_MP-RAGE_REPEAT :    1 subjects\n",
      "171. MP-RAGE + MP-RAGE_REPEAT + MPRAGE + MPRAGE_SENSE + MPRAGE_SENSE2 + MPRAGE_SENSE_repeat + SAG_MP-RAGE + SAG_MP-RAGE_REPEAT + Sagittal_3D_Accelerated_MPRAGE :    1 subjects\n",
      "172. Sagittal_3D_Accelerated_MPRAGE + Sagittal_3D_Accelerated_MPRAGE_REPEAT :    1 subjects\n",
      "173. MP-RAGE + MP-RAGE-REPEAT + MP-RAGE_REPEAT + MPRAGE + MPRAGE_Repeat + Sag_MPRAGE :    1 subjects\n",
      "174. MP-RAGE + MP-RAGE_Repeat + MP-RAGE__REPEAT + MPRAGE_3dtf + MPRAGE_3dtfe + MPRAGE_3dtferepeat + MPRAGE_repe + MPRAGE_repeat :    1 subjects\n",
      "175. MP-RAGE + MP-RAGE_REPEAT + MPRAGE_3dtf + MPRAGE_3dtfe + MPRAGE_repe + MPRAGE_repeat :    1 subjects\n",
      "176. MP-RAGE + MP-RAGE_REPEAT + MPRAGE_3dtfe + MPRAGE_3dtferepeat + MPRAGE_repeat + ___________MPRAGE_3dtf + ___________MPRAGE_repe :    1 subjects\n",
      "177. Accelerated_Sagittal_MPRAGE + Accelerated_Sagittal_MPRAGE_ND + MPRAGE + MPRAGE_GRAPPA2 + MPRAGE_Repeat :    1 subjects\n",
      "178. Accelerated_Sagittal_MPRAGE + Accelerated_Sagittal_MPRAGE_MSV21 + Accelerated_Sagittal_MPRAGE_ND :    1 subjects\n",
      "179. Sagittal_3D_Accelerated_MPRAGE + VWIP_Coronal_3D_Accelerated_MPRAGE    :    1 subjects\n",
      "180. 3D_MPRAGE + Accelerated_Sagittal_MPRAGE + Accelerated_Sagittal_MPRAGE__MSV21_ :    1 subjects\n",
      "181. Accelerated_Sagittal_MPRAGE + Accelerated_Sagittal_MPRAGE__MSV21_ + CS_Sagittal_MPRAGE__MSV22_ :    1 subjects\n",
      "\n",
      "üìà COMBINATION STATISTICS\n",
      "==================================================\n",
      "Total subjects: 1540\n",
      "Total unique scan types: 114\n",
      "Total unique combinations: 181\n",
      "Most common combination: Accelerated_Sagittal_MPRAGE__MSV21_ (301 subjects)\n",
      "Maximum scan types per subject: 12\n",
      "Subjects with 12 scan types: 100_S_1286\n",
      "Minimum scan types per subject: 1\n",
      "Subjects with 1 scan types: 002_S_6009, 002_S_6030, 002_S_6066, 002_S_6456, 002_S_6680...\n",
      "\n",
      "üîç EXAMPLES OF UNIQUE COMBINATIONS\n",
      "==================================================\n",
      " 1. Accelerated_Sagittal_MPRAGE__MSV21_ (301 subjects)\n",
      " 2. Accelerated_Sagittal_MPRAGE (184 subjects)\n",
      " 3. MPRAGE + MPRAGE_Repeat (126 subjects)\n",
      " 4. MPRAGE + MPRAGE_GRAPPA2 (119 subjects)\n",
      " 5. MP-RAGE + MP-RAGE_REPEAT (104 subjects)\n",
      " 6. Accelerated_Sagittal_MPRAGE + Accelerated_Sagittal_MPRAGE__MSV21_ (88 subjects)\n",
      " 7. MPRAGE + MPRAGE_SENSE2 (60 subjects)\n",
      " 8. Accelerated_Sagittal_MPRAGE + Accelerated_Sagittal_MPRAGE_ND (54 subjects)\n",
      " 9. Sagittal_3D_Accelerated_MPRAGE (42 subjects)\n",
      "10. Accelerated_Sagittal_MPRAGE__MSV22_ + HS_Sagittal_MPRAGE__MSV22_ (33 subjects)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "import itertools\n",
    "\n",
    "def analyze_scan_type_combinations(directory_path):\n",
    "    \"\"\"\n",
    "    Analyze unique combinations of scan types across all subjects.\n",
    "    \n",
    "    Args:\n",
    "        directory_path (str): Path to the directory containing subject folders\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with combination analysis results\n",
    "    \"\"\"\n",
    "    directory = Path(directory_path)\n",
    "    \n",
    "    if not directory.exists():\n",
    "        print(f\"‚ùå Directory not found: {directory}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"üîç Analyzing scan type combinations in: {directory}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Dictionary to store each subject's scan types\n",
    "    subject_scan_types = {}\n",
    "    # Counter for individual scan types\n",
    "    scan_type_counts = Counter()\n",
    "    # Counter for scan type combinations\n",
    "    combination_counts = Counter()\n",
    "    \n",
    "    # Get all subject directories\n",
    "    subject_dirs = [d for d in directory.iterdir() if d.is_dir()]\n",
    "    total_subjects = len(subject_dirs)\n",
    "    \n",
    "    print(f\"üìä Total subjects found: {total_subjects}\")\n",
    "    print()\n",
    "    \n",
    "    # Analyze each subject\n",
    "    for subject_dir in subject_dirs:\n",
    "        subject_id = subject_dir.name\n",
    "        scan_types = set()\n",
    "        \n",
    "        # Get all scan type directories for this subject\n",
    "        scan_type_dirs = [d for d in subject_dir.iterdir() if d.is_dir()]\n",
    "        \n",
    "        for scan_type_dir in scan_type_dirs:\n",
    "            scan_type = scan_type_dir.name\n",
    "            scan_types.add(scan_type)\n",
    "            scan_type_counts[scan_type] += 1\n",
    "        \n",
    "        subject_scan_types[subject_id] = scan_types\n",
    "        \n",
    "        # Create a sorted tuple for the combination (for consistent counting)\n",
    "        if scan_types:\n",
    "            combination = tuple(sorted(scan_types))\n",
    "            combination_counts[combination] += 1\n",
    "    \n",
    "    # Print individual scan type counts\n",
    "    print(\"üìã INDIVIDUAL SCAN TYPE COUNTS\")\n",
    "    print(\"=\" * 50)\n",
    "    for scan_type, count in scan_type_counts.most_common():\n",
    "        print(f\"{scan_type:<60} : {count:>4}\")\n",
    "    \n",
    "    print()\n",
    "    print(\"üîó UNIQUE SCAN TYPE COMBINATIONS\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Total unique combinations: {len(combination_counts)}\")\n",
    "    print()\n",
    "    \n",
    "    # Sort combinations by frequency\n",
    "    sorted_combinations = combination_counts.most_common()\n",
    "    \n",
    "    print(\"üìä Combination Counts (sorted by frequency):\")\n",
    "    print(\"-\" * 80)\n",
    "    for i, (combination, count) in enumerate(sorted_combinations, 1):\n",
    "        combination_str = \" + \".join(combination)\n",
    "        print(f\"{i:>3}. {combination_str:<70} : {count:>4} subjects\")\n",
    "    \n",
    "    print()\n",
    "    print(\"üìà COMBINATION STATISTICS\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Total subjects: {total_subjects}\")\n",
    "    print(f\"Total unique scan types: {len(scan_type_counts)}\")\n",
    "    print(f\"Total unique combinations: {len(combination_counts)}\")\n",
    "    \n",
    "    # Most common combination\n",
    "    if sorted_combinations:\n",
    "        most_common = sorted_combinations[0]\n",
    "        print(f\"Most common combination: {' + '.join(most_common[0])} ({most_common[1]} subjects)\")\n",
    "    \n",
    "    # Subjects with most/least scan types\n",
    "    subject_scan_counts = {subj: len(scan_types) for subj, scan_types in subject_scan_types.items()}\n",
    "    if subject_scan_counts:\n",
    "        max_count = max(subject_scan_counts.values())\n",
    "        min_count = min(subject_scan_counts.values())\n",
    "        max_subjects = [subj for subj, count in subject_scan_counts.items() if count == max_count]\n",
    "        min_subjects = [subj for subj, count in subject_scan_counts.items() if count == min_count]\n",
    "        \n",
    "        print(f\"Maximum scan types per subject: {max_count}\")\n",
    "        print(f\"Subjects with {max_count} scan types: {', '.join(max_subjects[:5])}{'...' if len(max_subjects) > 5 else ''}\")\n",
    "        print(f\"Minimum scan types per subject: {min_count}\")\n",
    "        print(f\"Subjects with {min_count} scan types: {', '.join(min_subjects[:5])}{'...' if len(min_subjects) > 5 else ''}\")\n",
    "    \n",
    "    # Show some examples of unique combinations\n",
    "    print()\n",
    "    print(\"üîç EXAMPLES OF UNIQUE COMBINATIONS\")\n",
    "    print(\"=\" * 50)\n",
    "    for i, (combination, count) in enumerate(sorted_combinations[:10], 1):\n",
    "        combination_str = \" + \".join(combination)\n",
    "        print(f\"{i:>2}. {combination_str} ({count} subjects)\")\n",
    "    \n",
    "    return {\n",
    "        'subject_scan_types': subject_scan_types,\n",
    "        'scan_type_counts': dict(scan_type_counts),\n",
    "        'combination_counts': dict(combination_counts),\n",
    "        'total_subjects': total_subjects,\n",
    "        'total_scan_types': len(scan_type_counts),\n",
    "        'total_combinations': len(combination_counts)\n",
    "    }\n",
    "\n",
    "# Run the analysis\n",
    "directory_path = r\"C:\\Users\\User\\github_repos\\AD_CN_all_available_data_final\\ADNI\"\n",
    "result = analyze_scan_type_combinations(directory_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1eef3f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Creating training dataset\n",
      "üìÇ Source: C:\\Users\\User\\github_repos\\AD_CN_all_available_data_final\\ADNI\n",
      "üìÅ Target: C:\\Users\\User\\github_repos\\AD_CN_train_v1\n",
      "üîÑ Resume mode: ON\n",
      "================================================================================\n",
      "üìä Total subjects found: 1540\n",
      "üîÑ Found 136 existing subjects, will skip if complete\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing subjects: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1540/1540 [08:06<00:00,  3.16subj/s, Copied=538, Skipped=136, Errors=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã SELECTION SUMMARY\n",
      "============================================================\n",
      "Subjects with 'MPRAGE' selected:         423\n",
      "Subjects with 'MP-RAGE' selected:        115\n",
      "Subjects excluded (no MPRAGE):           866\n",
      "------------------------------------------------------------\n",
      "Total subjects processed:               1540\n",
      "Total subjects selected:                 538\n",
      "\n",
      "üìÅ COPY OPERATIONS\n",
      "============================================================\n",
      "‚úÖ Successfully copied:                  538\n",
      "‚è≠ Skipped (already exists):             136\n",
      "‚ùå Errors:                                 0\n",
      "\n",
      "üìä COVERAGE STATISTICS\n",
      "============================================================\n",
      "Dataset coverage: 34.9% (538/1540)\n",
      "Processing time: 486.8 seconds\n",
      "\n",
      "üîç SAMPLE SELECTIONS (first 10 of each)\n",
      "============================================================\n",
      "Subjects with 'MPRAGE' selected:\n",
      "   1. 016_S_0991\n",
      "   2. 016_S_1263\n",
      "   3. 018_S_0043\n",
      "   4. 018_S_0055\n",
      "   5. 018_S_0286\n",
      "   6. 018_S_0335\n",
      "   7. 018_S_0369\n",
      "   8. 018_S_0425\n",
      "   9. 018_S_0633\n",
      "  10. 018_S_0682\n",
      "  ... and 413 more\n",
      "\n",
      "Subjects with 'MP-RAGE' selected:\n",
      "   1. 021_S_0159\n",
      "   2. 021_S_0337\n",
      "   3. 021_S_0343\n",
      "   4. 021_S_0642\n",
      "   5. 021_S_0647\n",
      "   6. 021_S_0984\n",
      "   7. 021_S_1109\n",
      "   8. 024_S_0985\n",
      "   9. 024_S_1063\n",
      "  10. 024_S_1171\n",
      "  ... and 105 more\n",
      "\n",
      "Subjects excluded: 866 (no MPRAGE available)\n",
      "\n",
      "üéØ Training dataset created at: C:\\Users\\User\\github_repos\\AD_CN_train_v1\n",
      "üìÅ Directory structure: C:\\Users\\User\\github_repos\\AD_CN_train_v1/<subject_id>/<scan_type>/...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def create_training_dataset(source_dir, target_dir, resume=True):\n",
    "    \"\"\"\n",
    "    Create training dataset by selecting one scan per subject with progress bar and resume capability.\n",
    "    Selection logic:\n",
    "    1. Look for 'MPRAGE' scan type\n",
    "    2. If not available, look for 'MP-RAGE' scan type\n",
    "    3. Copy the selected scan maintaining directory organization\n",
    "    \n",
    "    Args:\n",
    "        source_dir (str): Source directory containing subject folders\n",
    "        target_dir (str): Target directory for training dataset\n",
    "        resume (bool): Resume from where left off (skip existing files)\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with selection and copy results\n",
    "    \"\"\"\n",
    "    import shutil\n",
    "    from pathlib import Path\n",
    "    from tqdm import tqdm\n",
    "    import time\n",
    "    \n",
    "    source_path = Path(source_dir)\n",
    "    target_path = Path(target_dir)\n",
    "    \n",
    "    if not source_path.exists():\n",
    "        print(f\"‚ùå Source directory not found: {source_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Create target directory\n",
    "    target_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"üîç Creating training dataset\")\n",
    "    print(f\"üìÇ Source: {source_path}\")\n",
    "    print(f\"üìÅ Target: {target_path}\")\n",
    "    print(f\"üîÑ Resume mode: {'ON' if resume else 'OFF'}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Categories for tracking\n",
    "    subjects_with_mprage = []\n",
    "    subjects_with_mp_rage = []\n",
    "    subjects_with_neither = []\n",
    "    \n",
    "    # Copy statistics\n",
    "    copied_count = 0\n",
    "    skipped_count = 0\n",
    "    error_count = 0\n",
    "    \n",
    "    # Get all subject directories\n",
    "    subject_dirs = [d for d in source_path.iterdir() if d.is_dir()]\n",
    "    total_subjects = len(subject_dirs)\n",
    "    \n",
    "    print(f\"üìä Total subjects found: {total_subjects}\")\n",
    "    \n",
    "    # Check existing files for resume\n",
    "    if resume:\n",
    "        existing_subjects = set()\n",
    "        for existing_dir in target_path.iterdir():\n",
    "            if existing_dir.is_dir():\n",
    "                # Check if subject has scan type directory\n",
    "                scan_dirs = [d for d in existing_dir.iterdir() if d.is_dir()]\n",
    "                if scan_dirs:  # Has at least one scan type\n",
    "                    existing_subjects.add(existing_dir.name)\n",
    "        print(f\"üîÑ Found {len(existing_subjects)} existing subjects, will skip if complete\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Process each subject with progress bar\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with tqdm(total=total_subjects, desc=\"Processing subjects\", unit=\"subj\") as pbar:\n",
    "        for subject_dir in subject_dirs:\n",
    "            subject_id = subject_dir.name\n",
    "            \n",
    "            # Skip if already processed (resume mode)\n",
    "            if resume and subject_id in existing_subjects:\n",
    "                # Check if this subject has a complete scan\n",
    "                target_subject_dir = target_path / subject_id\n",
    "                scan_dirs = [d for d in target_subject_dir.iterdir() if d.is_dir()]\n",
    "                if scan_dirs:  # Has scan type directory\n",
    "                    skipped_count += 1\n",
    "                    pbar.set_postfix({\n",
    "                        'Copied': copied_count, \n",
    "                        'Skipped': skipped_count, \n",
    "                        'Errors': error_count\n",
    "                    })\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "            \n",
    "            scan_types = []\n",
    "            \n",
    "            # Get all scan type directories for this subject\n",
    "            scan_type_dirs = [d for d in subject_dir.iterdir() if d.is_dir()]\n",
    "            \n",
    "            for scan_type_dir in scan_type_dirs:\n",
    "                scan_type = scan_type_dir.name\n",
    "                scan_types.append(scan_type)\n",
    "            \n",
    "            # Selection logic: prioritize MPRAGE over MP-RAGE\n",
    "            selected_scan_type = None\n",
    "            \n",
    "            if 'MPRAGE' in scan_types:\n",
    "                selected_scan_type = 'MPRAGE'\n",
    "                subjects_with_mprage.append(subject_id)\n",
    "            elif 'MP-RAGE' in scan_types:\n",
    "                selected_scan_type = 'MP-RAGE'\n",
    "                subjects_with_mp_rage.append(subject_id)\n",
    "            else:\n",
    "                subjects_with_neither.append(subject_id)\n",
    "                pbar.set_postfix({\n",
    "                    'Copied': copied_count, \n",
    "                    'Skipped': skipped_count, \n",
    "                    'Errors': error_count\n",
    "                })\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "            \n",
    "            # Copy the selected scan type\n",
    "            source_scan_dir = subject_dir / selected_scan_type\n",
    "            target_subject_dir = target_path / subject_id\n",
    "            target_scan_dir = target_subject_dir / selected_scan_type\n",
    "            \n",
    "            try:\n",
    "                # Create target subject directory\n",
    "                target_subject_dir.mkdir(parents=True, exist_ok=True)\n",
    "                \n",
    "                # Copy the scan type directory\n",
    "                if target_scan_dir.exists():\n",
    "                    skipped_count += 1\n",
    "                else:\n",
    "                    shutil.copytree(source_scan_dir, target_scan_dir)\n",
    "                    copied_count += 1\n",
    "                    \n",
    "            except Exception as e:\n",
    "                error_count += 1\n",
    "                # Only log errors, not every operation\n",
    "                if error_count <= 5:  # Log first 5 errors\n",
    "                    print(f\"\\n‚ùå Error copying {subject_id}: {e}\")\n",
    "                elif error_count == 6:\n",
    "                    print(f\"\\n‚ö†Ô∏è  Additional errors will be counted but not displayed...\")\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                'Copied': copied_count, \n",
    "                'Skipped': skipped_count, \n",
    "                'Errors': error_count\n",
    "            })\n",
    "            pbar.update(1)\n",
    "    \n",
    "    # Calculate final statistics\n",
    "    total_selected = len(subjects_with_mprage) + len(subjects_with_mp_rage)\n",
    "    total_excluded = len(subjects_with_neither)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    print()\n",
    "    print(\"üìã SELECTION SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Subjects with 'MPRAGE' selected:        {len(subjects_with_mprage):>4}\")\n",
    "    print(f\"Subjects with 'MP-RAGE' selected:       {len(subjects_with_mp_rage):>4}\")\n",
    "    print(f\"Subjects excluded (no MPRAGE):          {len(subjects_with_neither):>4}\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"Total subjects processed:               {total_subjects:>4}\")\n",
    "    print(f\"Total subjects selected:                {total_selected:>4}\")\n",
    "    \n",
    "    print()\n",
    "    print(\"üìÅ COPY OPERATIONS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"‚úÖ Successfully copied:                 {copied_count:>4}\")\n",
    "    print(f\"‚è≠ Skipped (already exists):            {skipped_count:>4}\")\n",
    "    print(f\"‚ùå Errors:                              {error_count:>4}\")\n",
    "    \n",
    "    print()\n",
    "    print(\"üìä COVERAGE STATISTICS\")\n",
    "    print(\"=\" * 60)\n",
    "    coverage_percentage = (total_selected / total_subjects) * 100\n",
    "    print(f\"Dataset coverage: {coverage_percentage:.1f}% ({total_selected}/{total_subjects})\")\n",
    "    print(f\"Processing time: {elapsed_time:.1f} seconds\")\n",
    "    \n",
    "    # Show some examples (only if not too many)\n",
    "    if total_selected <= 20:\n",
    "        print()\n",
    "        print(\"üîç ALL SELECTIONS\")\n",
    "        print(\"=\" * 60)\n",
    "        print(\"Subjects with 'MPRAGE' selected:\")\n",
    "        for i, subj in enumerate(subjects_with_mprage, 1):\n",
    "            print(f\"  {i:>2}. {subj}\")\n",
    "        \n",
    "        print(\"\\nSubjects with 'MP-RAGE' selected:\")\n",
    "        for i, subj in enumerate(subjects_with_mp_rage, 1):\n",
    "            print(f\"  {i:>2}. {subj}\")\n",
    "    else:\n",
    "        print()\n",
    "        print(\"üîç SAMPLE SELECTIONS (first 10 of each)\")\n",
    "        print(\"=\" * 60)\n",
    "        print(\"Subjects with 'MPRAGE' selected:\")\n",
    "        for i, subj in enumerate(subjects_with_mprage[:10], 1):\n",
    "            print(f\"  {i:>2}. {subj}\")\n",
    "        if len(subjects_with_mprage) > 10:\n",
    "            print(f\"  ... and {len(subjects_with_mprage) - 10} more\")\n",
    "        \n",
    "        print(\"\\nSubjects with 'MP-RAGE' selected:\")\n",
    "        for i, subj in enumerate(subjects_with_mp_rage[:10], 1):\n",
    "            print(f\"  {i:>2}. {subj}\")\n",
    "        if len(subjects_with_mp_rage) > 10:\n",
    "            print(f\"  ... and {len(subjects_with_mp_rage) - 10} more\")\n",
    "    \n",
    "    if subjects_with_neither and len(subjects_with_neither) <= 10:\n",
    "        print(f\"\\nSubjects excluded (no MPRAGE):\")\n",
    "        for i, subj in enumerate(subjects_with_neither, 1):\n",
    "            print(f\"  {i:>2}. {subj}\")\n",
    "    elif subjects_with_neither:\n",
    "        print(f\"\\nSubjects excluded: {len(subjects_with_neither)} (no MPRAGE available)\")\n",
    "    \n",
    "    print()\n",
    "    print(f\"üéØ Training dataset created at: {target_path}\")\n",
    "    print(f\"üìÅ Directory structure: {target_path}/<subject_id>/<scan_type>/...\")\n",
    "    \n",
    "    return {\n",
    "        'total_subjects': total_subjects,\n",
    "        'subjects_with_mprage': subjects_with_mprage,\n",
    "        'subjects_with_mp_rage': subjects_with_mp_rage,\n",
    "        'subjects_with_neither': subjects_with_neither,\n",
    "        'total_selected': total_selected,\n",
    "        'total_excluded': total_excluded,\n",
    "        'copied_count': copied_count,\n",
    "        'skipped_count': skipped_count,\n",
    "        'error_count': error_count,\n",
    "        'coverage_percentage': coverage_percentage,\n",
    "        'processing_time': elapsed_time,\n",
    "        'target_directory': str(target_path)\n",
    "    }\n",
    "\n",
    "# Create training dataset\n",
    "source_directory = r\"C:\\Users\\User\\github_repos\\AD_CN_all_available_data_final\\ADNI\"\n",
    "target_directory = r\"C:\\Users\\User\\github_repos\\AD_CN_train_v1\"\n",
    "\n",
    "training_result = create_training_dataset(source_directory, target_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bdd121aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Creating training dataset metadata...\n",
      "============================================================\n",
      "üìÇ Reading CSV: C:\\Users\\User\\github_repos\\AD_CN_all_available_data_final\\AD_CN_all_available_data.csv\n",
      "üìä CSV loaded: 31182 records\n",
      "üîç Filtering out unwanted scan types...\n",
      "üìä After filtering: 15852 records (removed 15330 unwanted scans)\n",
      "üìÅ Scanning training directory: C:\\Users\\User\\github_repos\\AD_CN_train_v1\n",
      "üìä Found 674 subjects to process\n",
      "\n",
      "üîç Scanning training scans (by Image Data ID)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning subjects: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 674/674 [00:05<00:00, 117.14subj/s, Scans found=2968]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Found 2968 unique training scans (3D images)\n",
      "\n",
      "üîó Matching training scans with CSV metadata by Image Data ID...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matching scans:   1%|          | 34/2968 [00:00<00:14, 201.67scan/s, Scans matched=35, Total scans=35]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç INSPECTION #1\n",
      "------------------------------------------------------------\n",
      "Training Scan: 002_S_0295\\MPRAGE\\2011-06-02_07_58_50.0\\I238627\n",
      "Subject ID: 002_S_0295\n",
      "Scan Type: MPRAGE\n",
      "Timestamp: 2011-06-02_07_58_50.0\n",
      "Image Data ID: I238627\n",
      "DICOM files count: 170\n",
      "CSV Match:\n",
      "  - Description: MPRAGE\n",
      "  - Acq Date: 6/02/2011\n",
      "  - Group: CN\n",
      "  - Sex: M\n",
      "  - Age: 90\n",
      "  - Visit: v06\n",
      "------------------------------------------------------------\n",
      "\n",
      "üîç INSPECTION #2\n",
      "------------------------------------------------------------\n",
      "Training Scan: 002_S_0295\\MPRAGE\\2012-05-10_15_44_50.0\\I303066\n",
      "Subject ID: 002_S_0295\n",
      "Scan Type: MPRAGE\n",
      "Timestamp: 2012-05-10_15_44_50.0\n",
      "Image Data ID: I303066\n",
      "DICOM files count: 170\n",
      "CSV Match:\n",
      "  - Description: MPRAGE\n",
      "  - Acq Date: 5/10/2012\n",
      "  - Group: CN\n",
      "  - Sex: M\n",
      "  - Age: 91\n",
      "  - Visit: v11\n",
      "------------------------------------------------------------\n",
      "\n",
      "üîç INSPECTION #3\n",
      "------------------------------------------------------------\n",
      "Training Scan: 002_S_0413\\MPRAGE\\2006-11-15_14_23_26.0\\I30119\n",
      "Subject ID: 002_S_0413\n",
      "Scan Type: MPRAGE\n",
      "Timestamp: 2006-11-15_14_23_26.0\n",
      "Image Data ID: I30119\n",
      "DICOM files count: 170\n",
      "CSV Match:\n",
      "  - Description: MPRAGE\n",
      "  - Acq Date: 11/15/2006\n",
      "  - Group: CN\n",
      "  - Sex: F\n",
      "  - Age: 77\n",
      "  - Visit: m06\n",
      "------------------------------------------------------------\n",
      "\n",
      "üîç INSPECTION #4\n",
      "------------------------------------------------------------\n",
      "Training Scan: 002_S_0413\\MPRAGE\\2006-11-15_14_34_44.0\\I30118\n",
      "Subject ID: 002_S_0413\n",
      "Scan Type: MPRAGE\n",
      "Timestamp: 2006-11-15_14_34_44.0\n",
      "Image Data ID: I30118\n",
      "DICOM files count: 170\n",
      "CSV Match:\n",
      "  - Description: MPRAGE\n",
      "  - Acq Date: 11/15/2006\n",
      "  - Group: CN\n",
      "  - Sex: F\n",
      "  - Age: 77\n",
      "  - Visit: m06\n",
      "------------------------------------------------------------\n",
      "\n",
      "üîç INSPECTION #5\n",
      "------------------------------------------------------------\n",
      "Training Scan: 002_S_0413\\MPRAGE\\2007-06-01_07_57_43.0\\I55782\n",
      "Subject ID: 002_S_0413\n",
      "Scan Type: MPRAGE\n",
      "Timestamp: 2007-06-01_07_57_43.0\n",
      "Image Data ID: I55782\n",
      "DICOM files count: 170\n",
      "CSV Match:\n",
      "  - Description: MPRAGE\n",
      "  - Acq Date: 6/01/2007\n",
      "  - Group: CN\n",
      "  - Sex: F\n",
      "  - Age: 77\n",
      "  - Visit: m12\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matching scans: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2968/2968 [00:15<00:00, 192.50scan/s, Scans matched=2968, Total scans=2968]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created metadata for 2968 files\n",
      "\n",
      "üìä METADATA SUMMARY\n",
      "============================================================\n",
      "Total training scans (3D images): 2968\n",
      "Total DICOM files: 2968\n",
      "Files with metadata: 2968\n",
      "Coverage: 100.0% (exact Image Data ID matching)\n",
      "\n",
      "Group distribution:\n",
      "  CN: 1989\n",
      "  AD: 979\n",
      "\n",
      "Scan type distribution:\n",
      "  MPRAGE: 2276\n",
      "  MP-RAGE: 692\n",
      "\n",
      "Matching quality:\n",
      "  Method: Exact Image Data ID matching\n",
      "  Accuracy: 100% (no fuzzy matching needed)\n",
      "\n",
      "üíæ Saving metadata to: C:\\Users\\User\\github_repos\\AD_CN_train_v1\\metadata.csv\n",
      "‚úÖ Metadata saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "def create_training_metadata(csv_path, training_dir, output_path=None):\n",
    "    \"\"\"\n",
    "    Create metadata for training dataset by matching CSV data with training directory structure.\n",
    "    \n",
    "    Args:\n",
    "        csv_path (str): Path to the CSV file with metadata\n",
    "        training_dir (str): Path to training dataset directory\n",
    "        output_path (str): Path to save metadata CSV (optional)\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Training dataset metadata\n",
    "    \"\"\"\n",
    "    print(\"üîç Creating training dataset metadata...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Read CSV file\n",
    "    print(f\"üìÇ Reading CSV: {csv_path}\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"üìä CSV loaded: {len(df)} records\")\n",
    "    \n",
    "    # Filter out unwanted scan types to speed up processing\n",
    "    print(\"üîç Filtering out unwanted scan types...\")\n",
    "    unwanted_keywords = ['mapping', 'localizer', 'survey', 'scout']\n",
    "    \n",
    "    # Create a mask to exclude rows with unwanted keywords in Description\n",
    "    mask = ~df['Description'].str.lower().str.contains('|'.join(unwanted_keywords), na=False)\n",
    "    df_filtered = df[mask].copy()\n",
    "    \n",
    "    print(f\"üìä After filtering: {len(df_filtered)} records (removed {len(df) - len(df_filtered)} unwanted scans)\")\n",
    "    \n",
    "    # Use filtered dataframe for processing\n",
    "    df = df_filtered\n",
    "    \n",
    "    # Load training directory structure\n",
    "    print(f\"üìÅ Scanning training directory: {training_dir}\")\n",
    "    training_path = Path(training_dir)\n",
    "    \n",
    "    # Get all subject directories first for progress tracking\n",
    "    subject_dirs = [d for d in training_path.iterdir() if d.is_dir()]\n",
    "    total_subjects = len(subject_dirs)\n",
    "    \n",
    "    print(f\"üìä Found {total_subjects} subjects to process\")\n",
    "    \n",
    "    # Collect unique Image Data IDs (3D scans) instead of individual files\n",
    "    training_scans = []\n",
    "    \n",
    "    print(\"\\nüîç Scanning training scans (by Image Data ID)...\")\n",
    "    with tqdm(total=total_subjects, desc=\"Scanning subjects\", unit=\"subj\") as pbar:\n",
    "        for subject_dir in subject_dirs:\n",
    "            subject_id = subject_dir.name\n",
    "            \n",
    "            # Find scan type directories\n",
    "            for scan_dir in subject_dir.iterdir():\n",
    "                if not scan_dir.is_dir():\n",
    "                    continue\n",
    "                    \n",
    "                scan_type = scan_dir.name\n",
    "                \n",
    "                # Find timestamp directories\n",
    "                for timestamp_dir in scan_dir.iterdir():\n",
    "                    if not timestamp_dir.is_dir():\n",
    "                        continue\n",
    "                        \n",
    "                    timestamp_str = timestamp_dir.name\n",
    "                    \n",
    "                    # Find Image Data ID directories (I238627, etc.)\n",
    "                    for image_id_dir in timestamp_dir.iterdir():\n",
    "                        if not image_id_dir.is_dir():\n",
    "                            continue\n",
    "                            \n",
    "                        image_data_id = image_id_dir.name\n",
    "                        \n",
    "                        # Count DICOM files in this scan\n",
    "                        dcm_files = list(image_id_dir.glob(\"*.dcm\"))\n",
    "                        if dcm_files:  # Only include if has DICOM files\n",
    "                            # Get one representative file path\n",
    "                            sample_file = dcm_files[0]\n",
    "                            training_scans.append({\n",
    "                                'subject_id': subject_id,\n",
    "                                'scan_type': scan_type,\n",
    "                                'timestamp': timestamp_str,\n",
    "                                'image_data_id': image_data_id,\n",
    "                                'dcm_count': len(dcm_files),\n",
    "                                'sample_file_path': str(sample_file),\n",
    "                                'relative_scan_path': str(image_id_dir.relative_to(training_path))\n",
    "                            })\n",
    "            \n",
    "            pbar.set_postfix({'Scans found': len(training_scans)})\n",
    "            pbar.update(1)\n",
    "    \n",
    "    print(f\"üìä Found {len(training_scans)} unique training scans (3D images)\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    training_df = pd.DataFrame(training_scans)\n",
    "    \n",
    "    # Create metadata by matching using Image Data ID (much faster!)\n",
    "    metadata_records = []\n",
    "    \n",
    "    print(\"\\nüîó Matching training scans with CSV metadata by Image Data ID...\")\n",
    "    \n",
    "    # Visual inspection counter\n",
    "    inspection_count = 0\n",
    "    \n",
    "    with tqdm(total=len(training_df), desc=\"Matching scans\", unit=\"scan\") as pbar:\n",
    "        for idx, (_, training_scan) in enumerate(training_df.iterrows()):\n",
    "            subject_id = training_scan['subject_id']\n",
    "            image_data_id = training_scan['image_data_id']\n",
    "            \n",
    "            # Find matching record in CSV by Image Data ID (exact match - much faster!)\n",
    "            matching_record = df[df['Image Data ID'] == image_data_id]\n",
    "            \n",
    "            if len(matching_record) == 0:\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "            \n",
    "            # Should be exactly one match since Image Data ID is unique\n",
    "            best_match = matching_record.iloc[0]\n",
    "            \n",
    "            # Visual inspection for first 5 scans\n",
    "            if inspection_count < 5:\n",
    "                print(f\"\\nüîç INSPECTION #{inspection_count + 1}\")\n",
    "                print(\"-\" * 60)\n",
    "                print(f\"Training Scan: {training_scan['relative_scan_path']}\")\n",
    "                print(f\"Subject ID: {subject_id}\")\n",
    "                print(f\"Scan Type: {training_scan['scan_type']}\")\n",
    "                print(f\"Timestamp: {training_scan['timestamp']}\")\n",
    "                print(f\"Image Data ID: {image_data_id}\")\n",
    "                print(f\"DICOM files count: {training_scan['dcm_count']}\")\n",
    "                print(f\"CSV Match:\")\n",
    "                print(f\"  - Description: {best_match['Description']}\")\n",
    "                print(f\"  - Acq Date: {best_match['Acq Date']}\")\n",
    "                print(f\"  - Group: {best_match['Group']}\")\n",
    "                print(f\"  - Sex: {best_match['Sex']}\")\n",
    "                print(f\"  - Age: {best_match['Age']}\")\n",
    "                print(f\"  - Visit: {best_match['Visit']}\")\n",
    "                print(\"-\" * 60)\n",
    "                inspection_count += 1\n",
    "            \n",
    "            # Create one metadata record per 3D scan (not per DICOM file)\n",
    "            metadata_record = {\n",
    "                'Subject': subject_id,\n",
    "                'ScanPath': training_scan['relative_scan_path'],\n",
    "                'ScanType': training_scan['scan_type'],\n",
    "                'Timestamp': training_scan['timestamp'],\n",
    "                'ImageDataID': image_data_id,\n",
    "                'DicomCount': training_scan['dcm_count'],\n",
    "                'Group': best_match['Group'],\n",
    "                'Sex': best_match['Sex'],\n",
    "                'Age': best_match['Age'],\n",
    "                'Visit': best_match['Visit'],\n",
    "                'Modality': best_match['Modality'],\n",
    "                'Description': best_match['Description'],\n",
    "                'Type': best_match['Type'],\n",
    "                'AcqDate': best_match['Acq Date'],\n",
    "                'Format': best_match['Format']\n",
    "            }\n",
    "            \n",
    "            metadata_records.append(metadata_record)\n",
    "            \n",
    "            pbar.set_postfix({'Scans matched': idx + 1, 'Total scans': len(metadata_records)})\n",
    "            pbar.update(1)\n",
    "    \n",
    "    # Create final metadata DataFrame\n",
    "    metadata_df = pd.DataFrame(metadata_records)\n",
    "    \n",
    "    print(f\"‚úÖ Created metadata for {len(metadata_df)} 3D scans\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\nüìä METADATA SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total training scans (3D images): {len(training_df)}\")\n",
    "    print(f\"Scans with metadata: {len(metadata_df)}\")\n",
    "    print(f\"Total DICOM files: {metadata_df['DicomCount'].sum()}\")\n",
    "    print(f\"Coverage: 100.0% (exact Image Data ID matching)\")\n",
    "    \n",
    "    # Group distribution\n",
    "    if 'Group' in metadata_df.columns:\n",
    "        print(f\"\\nGroup distribution:\")\n",
    "        group_counts = metadata_df['Group'].value_counts()\n",
    "        for group, count in group_counts.items():\n",
    "            print(f\"  {group}: {count}\")\n",
    "    \n",
    "    # Scan type distribution\n",
    "    if 'ScanType' in metadata_df.columns:\n",
    "        print(f\"\\nScan type distribution:\")\n",
    "        scan_counts = metadata_df['ScanType'].value_counts()\n",
    "        for scan_type, count in scan_counts.items():\n",
    "            print(f\"  {scan_type}: {count}\")\n",
    "    \n",
    "    # Matching quality (Image Data ID is exact match)\n",
    "    print(f\"\\nMatching quality:\")\n",
    "    print(f\"  Method: Exact Image Data ID matching\")\n",
    "    print(f\"  Accuracy: 100% (no fuzzy matching needed)\")\n",
    "    \n",
    "    # Save metadata if output path provided\n",
    "    if output_path:\n",
    "        print(f\"\\nüíæ Saving metadata to: {output_path}\")\n",
    "        metadata_df.to_csv(output_path, index=False)\n",
    "        print(f\"‚úÖ Metadata saved successfully!\")\n",
    "    \n",
    "    return metadata_df\n",
    "\n",
    "# Run the metadata creation\n",
    "csv_file = r\"C:\\Users\\User\\github_repos\\AD_CN_all_available_data_final\\AD_CN_all_available_data.csv\"\n",
    "training_directory = r\"C:\\Users\\User\\github_repos\\AD_CN_train_v1\"\n",
    "output_file = r\"C:\\Users\\User\\github_repos\\AD_CN_train_v1\\metadata.csv\"\n",
    "\n",
    "metadata = create_training_metadata(csv_file, training_directory, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f3182f",
   "metadata": {},
   "source": [
    "# Analyze demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b58256a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Loading metadata...\n",
      "üìä Total scans in dataset: 674\n",
      "üìä Unique subjects: 674\n",
      "\n",
      "============================================================\n",
      "üìã 1. AD vs CN CLASS DISTRIBUTION\n",
      "============================================================\n",
      "CN (Cognitively Normal): 385 scans\n",
      "AD (Alzheimer's Disease): 289 scans\n",
      "CN: 57.1%\n",
      "AD: 42.9%\n",
      "\n",
      "============================================================\n",
      "üìã 2. SEX DISTRIBUTION BY GROUP\n",
      "============================================================\n",
      "\n",
      "Sex Distribution:\n",
      "Sex      F    M  All\n",
      "Group               \n",
      "AD     135  154  289\n",
      "CN     194  191  385\n",
      "All    329  345  674\n",
      "\n",
      "Detailed Breakdown:\n",
      "\n",
      "CN Group:\n",
      "  Male: 191 scans (49.6%)\n",
      "  Female: 194 scans (50.4%)\n",
      "\n",
      "AD Group:\n",
      "  Male: 154 scans (53.3%)\n",
      "  Female: 135 scans (46.7%)\n",
      "\n",
      "============================================================\n",
      "üìã 3. AGE STATISTICS BY GROUP\n",
      "============================================================\n",
      "\n",
      "CN Group:\n",
      "  Mean Age: 76.5 years\n",
      "  Std Dev: 6.3 years\n",
      "  Age Range: 57 - 95 years\n",
      "  Sample Size: 385 scans\n",
      "\n",
      "AD Group:\n",
      "  Mean Age: 76.0 years\n",
      "  Std Dev: 7.7 years\n",
      "  Age Range: 56 - 91 years\n",
      "  Sample Size: 289 scans\n",
      "\n",
      "Overall Dataset:\n",
      "  Mean Age: 76.2 years\n",
      "  Std Dev: 6.9 years\n",
      "  Age Range: 56 - 95 years\n",
      "\n",
      "============================================================\n",
      "üìã ADDITIONAL INSIGHTS\n",
      "============================================================\n",
      "\n",
      "Scan Type Distribution:\n",
      "  MPRAGE: 527 scans\n",
      "  MP-RAGE: 147 scans\n",
      "\n",
      "Top 10 Visits:\n",
      "  sc: 111 scans\n",
      "  v02: 90 scans\n",
      "  m06: 90 scans\n",
      "  m12: 70 scans\n",
      "  v11: 57 scans\n",
      "  v04: 46 scans\n",
      "  m24: 46 scans\n",
      "  v05: 43 scans\n",
      "  v21: 40 scans\n",
      "  m36: 26 scans\n",
      "\n",
      "‚úÖ Analysis completed!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def analyze_demographics(csv_path):\n",
    "    \"\"\"\n",
    "    Analyze demographic statistics from the training metadata CSV\n",
    "    \"\"\"\n",
    "    print(\"üîç Loading metadata...\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    print(f\"üìä Total scans in dataset: {len(df)}\")\n",
    "    print(f\"üìä Unique subjects: {df['Subject'].nunique()}\")\n",
    "    \n",
    "    # 1. AD vs CN class count\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìã 1. AD vs CN CLASS DISTRIBUTION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    group_counts = df['Group'].value_counts()\n",
    "    print(f\"CN (Cognitively Normal): {group_counts.get('CN', 0)} scans\")\n",
    "    print(f\"AD (Alzheimer's Disease): {group_counts.get('AD', 0)} scans\")\n",
    "    \n",
    "    # Calculate percentages\n",
    "    total_scans = len(df)\n",
    "    cn_pct = (group_counts.get('CN', 0) / total_scans) * 100\n",
    "    ad_pct = (group_counts.get('AD', 0) / total_scans) * 100\n",
    "    print(f\"CN: {cn_pct:.1f}%\")\n",
    "    print(f\"AD: {ad_pct:.1f}%\")\n",
    "    \n",
    "    # 2. Male vs Female distribution for AD and CN\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìã 2. SEX DISTRIBUTION BY GROUP\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Cross-tabulation of Group and Sex\n",
    "    cross_tab = pd.crosstab(df['Group'], df['Sex'], margins=True)\n",
    "    print(\"\\nSex Distribution:\")\n",
    "    print(cross_tab)\n",
    "    \n",
    "    # Detailed breakdown\n",
    "    print(\"\\nDetailed Breakdown:\")\n",
    "    for group in ['CN', 'AD']:\n",
    "        group_data = df[df['Group'] == group]\n",
    "        if len(group_data) > 0:\n",
    "            male_count = len(group_data[group_data['Sex'] == 'M'])\n",
    "            female_count = len(group_data[group_data['Sex'] == 'F'])\n",
    "            total_group = len(group_data)\n",
    "            \n",
    "            print(f\"\\n{group} Group:\")\n",
    "            print(f\"  Male: {male_count} scans ({male_count/total_group*100:.1f}%)\")\n",
    "            print(f\"  Female: {female_count} scans ({female_count/total_group*100:.1f}%)\")\n",
    "    \n",
    "    # 3. Age statistics for AD and CN\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìã 3. AGE STATISTICS BY GROUP\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for group in ['CN', 'AD']:\n",
    "        group_data = df[df['Group'] == group]\n",
    "        if len(group_data) > 0:\n",
    "            ages = group_data['Age'].dropna()\n",
    "            if len(ages) > 0:\n",
    "                mean_age = ages.mean()\n",
    "                std_age = ages.std()\n",
    "                min_age = ages.min()\n",
    "                max_age = ages.max()\n",
    "                \n",
    "                print(f\"\\n{group} Group:\")\n",
    "                print(f\"  Mean Age: {mean_age:.1f} years\")\n",
    "                print(f\"  Std Dev: {std_age:.1f} years\")\n",
    "                print(f\"  Age Range: {min_age:.0f} - {max_age:.0f} years\")\n",
    "                print(f\"  Sample Size: {len(ages)} scans\")\n",
    "    \n",
    "    # Overall age statistics\n",
    "    print(f\"\\nOverall Dataset:\")\n",
    "    overall_ages = df['Age'].dropna()\n",
    "    if len(overall_ages) > 0:\n",
    "        print(f\"  Mean Age: {overall_ages.mean():.1f} years\")\n",
    "        print(f\"  Std Dev: {overall_ages.std():.1f} years\")\n",
    "        print(f\"  Age Range: {overall_ages.min():.0f} - {overall_ages.max():.0f} years\")\n",
    "    \n",
    "    # Additional insights\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìã ADDITIONAL INSIGHTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Scan type distribution\n",
    "    scan_type_counts = df['ScanType'].value_counts()\n",
    "    print(f\"\\nScan Type Distribution:\")\n",
    "    for scan_type, count in scan_type_counts.items():\n",
    "        print(f\"  {scan_type}: {count} scans\")\n",
    "    \n",
    "    # Visit distribution\n",
    "    visit_counts = df['Visit'].value_counts()\n",
    "    print(f\"\\nTop 10 Visits:\")\n",
    "    for visit, count in visit_counts.head(10).items():\n",
    "        print(f\"  {visit}: {count} scans\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    csv_path = r\"C:\\Users\\User\\github_repos\\AD_CN_train_v2\\metadata.csv\"\n",
    "    df = analyze_demographics(csv_path)\n",
    "    print(\"\\n‚úÖ Analysis completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a73f08cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Loading metadata...\n",
      "üìä Total scans in dataset: 2968\n",
      "üìä Unique subjects: 674\n",
      "\n",
      "üìã SUBJECT-GROUP ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Subjects by number of groups:\n",
      "  1 group: 674 subjects\n",
      "\n",
      "‚úÖ No subjects found with multiple groups\n",
      "All subjects belong to only one group (AD or CN)\n",
      "\n",
      "üìä SUMMARY:\n",
      "============================================================\n",
      "Total unique subjects: 674\n",
      "Subjects with single group: 674\n",
      "Subjects with multiple groups: 0\n"
     ]
    }
   ],
   "source": [
    "def analyze_subject_groups(csv_path):\n",
    "    \"\"\"\n",
    "    Analyze no of unique subjects and check for subjects with both AD and CN groups\n",
    "    \"\"\"\n",
    "    print(\"üîç Loading metadata...\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    print(f\"üìä Total scans in dataset: {len(df)}\")\n",
    "    \n",
    "    # Count unique subjects\n",
    "    unique_subjects = df['Subject'].nunique()\n",
    "    print(f\"üìä Unique subjects: {unique_subjects}\")\n",
    "    \n",
    "    # Analyze subject-group combinations\n",
    "    subject_groups = df.groupby('Subject')['Group'].unique().reset_index()\n",
    "    subject_groups['GroupCount'] = subject_groups['Group'].apply(len)\n",
    "    subject_groups['Groups'] = subject_groups['Group'].apply(lambda x: ', '.join(sorted(x)))\n",
    "    \n",
    "    print(f\"\\nüìã SUBJECT-GROUP ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Count subjects by number of groups\n",
    "    group_counts = subject_groups['GroupCount'].value_counts().sort_index()\n",
    "    print(f\"\\nSubjects by number of groups:\")\n",
    "    for num_groups, count in group_counts.items():\n",
    "        group_type = \"group\" if num_groups == 1 else \"groups\"\n",
    "        print(f\"  {num_groups} {group_type}: {count} subjects\")\n",
    "    \n",
    "    # Find subjects with both AD and CN\n",
    "    subjects_with_both = subject_groups[subject_groups['GroupCount'] > 1]\n",
    "    \n",
    "    if len(subjects_with_both) > 0:\n",
    "        print(f\"\\nüîç SUBJECTS WITH MULTIPLE GROUPS ({len(subjects_with_both)} subjects):\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for _, row in subjects_with_both.iterrows():\n",
    "            subject_id = row['Subject']\n",
    "            groups = row['Groups']\n",
    "            group_list = row['Group']\n",
    "            \n",
    "            # Get scan counts for this subject\n",
    "            subject_data = df[df['Subject'] == subject_id]\n",
    "            scan_counts = subject_data['Group'].value_counts()\n",
    "            \n",
    "            print(f\"Subject {subject_id}:\")\n",
    "            print(f\"  Groups: {groups}\")\n",
    "            for group in group_list:\n",
    "                count = scan_counts.get(group, 0)\n",
    "                print(f\"    {group}: {count} scans\")\n",
    "            print()\n",
    "    else:\n",
    "        print(f\"\\n‚úÖ No subjects found with multiple groups\")\n",
    "        print(\"All subjects belong to only one group (AD or CN)\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"\\nüìä SUMMARY:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total unique subjects: {unique_subjects}\")\n",
    "    print(f\"Subjects with single group: {len(subject_groups[subject_groups['GroupCount'] == 1])}\")\n",
    "    print(f\"Subjects with multiple groups: {len(subjects_with_both)}\")\n",
    "    \n",
    "    if len(subjects_with_both) > 0:\n",
    "        print(f\"Percentage with multiple groups: {len(subjects_with_both)/unique_subjects*100:.1f}%\")\n",
    "    \n",
    "    return subject_groups, subjects_with_both\n",
    "\n",
    "# Run the analysis\n",
    "csv_path = r\"C:\\Users\\User\\github_repos\\AD_CN_train_v1\\metadata.csv\"\n",
    "subject_groups, subjects_with_both = analyze_subject_groups(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b7ba81",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Given there is no subjects with MPRAGE or MP-RAGE that has change in labels is CN to AD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7097a926",
   "metadata": {},
   "source": [
    "# Select 1 scan per 'Subject'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9cd8f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Creating single-scan dataset...\n",
      "============================================================\n",
      "üîç Loading metadata...\n",
      "üìä Original dataset: 2968 scans from 674 subjects\n",
      "\n",
      "üéØ Selecting scans using 'random' strategy...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing subjects: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 674/674 [00:00<00:00, 888.11it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Selected 674 scans (1 per subject)\n",
      "\n",
      "üìä SELECTION SUMMARY:\n",
      "==================================================\n",
      "Original scans: 2968\n",
      "Selected scans: 674\n",
      "Reduction: 2294 scans removed\n",
      "\n",
      "Selected dataset distribution:\n",
      "  CN: 385 subjects\n",
      "  AD: 289 subjects\n",
      "üìÅ Copying selected scans...\n",
      "   Source: C:\\Users\\User\\github_repos\\AD_CN_train_v1\n",
      "   Target: C:\\Users\\User\\github_repos\\AD_CN_train_v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying scans: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 674/674 [03:54<00:00,  2.88it/s, Copied=644, Skipped=30, Errors=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Copy completed!\n",
      "   Copied: 644 scans\n",
      "   Skipped: 30 scans (already exist)\n",
      "   Errors: 0 scans\n",
      "üíæ Saved metadata: C:\\Users\\User\\github_repos\\AD_CN_train_v2\\metadata.csv\n",
      "\n",
      "üéâ DATASET CREATION COMPLETE!\n",
      "============================================================\n",
      "Strategy used: random\n",
      "Original dataset: 2968 scans\n",
      "New dataset: 674 scans (1 per subject)\n",
      "Files copied: 644\n",
      "Target directory: C:\\Users\\User\\github_repos\\AD_CN_train_v2\n",
      "Metadata file: C:\\Users\\User\\github_repos\\AD_CN_train_v2\\metadata.csv\n",
      "\n",
      "‚úÖ Single-scan dataset creation completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def select_one_scan_per_subject(csv_path, selection_strategy='random'):\n",
    "    \"\"\"\n",
    "    Select one scan per unique subject based on selection strategy\n",
    "    \n",
    "    Args:\n",
    "        csv_path: Path to metadata CSV\n",
    "        selection_strategy: 'random', 'earliest', 'latest', or 'baseline'\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with one scan per subject\n",
    "    \"\"\"\n",
    "    print(\"üîç Loading metadata...\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    print(f\"üìä Original dataset: {len(df)} scans from {df['Subject'].nunique()} subjects\")\n",
    "    \n",
    "    # Convert timestamp to datetime for proper sorting\n",
    "    df['TimestampDateTime'] = pd.to_datetime(df['Timestamp'], format='%Y-%m-%d_%H_%M_%S.%f')\n",
    "    \n",
    "    selected_scans = []\n",
    "    \n",
    "    print(f\"\\nüéØ Selecting scans using '{selection_strategy}' strategy...\")\n",
    "    \n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Group by subject and select one scan per subject\n",
    "    for subject_id, subject_data in tqdm(df.groupby('Subject'), desc=\"Processing subjects\"):\n",
    "        if selection_strategy == 'random':\n",
    "            # Select random scan\n",
    "            selected_scan = subject_data.sample(n=1, random_state=np.random.randint(0, 10000)).iloc[0]\n",
    "        elif selection_strategy == 'earliest':\n",
    "            # Select scan with earliest timestamp\n",
    "            selected_scan = subject_data.loc[subject_data['TimestampDateTime'].idxmin()]\n",
    "        elif selection_strategy == 'latest':\n",
    "            # Select scan with latest timestamp\n",
    "            selected_scan = subject_data.loc[subject_data['TimestampDateTime'].idxmax()]\n",
    "        elif selection_strategy == 'baseline':\n",
    "            # Prefer baseline visits (bl, m00, sc) then earliest\n",
    "            baseline_visits = subject_data[subject_data['Visit'].str.lower().isin(['bl', 'm00', 'sc'])]\n",
    "            if len(baseline_visits) > 0:\n",
    "                selected_scan = baseline_visits.loc[baseline_visits['TimestampDateTime'].idxmin()]\n",
    "            else:\n",
    "                selected_scan = subject_data.loc[subject_data['TimestampDateTime'].idxmin()]\n",
    "        else:\n",
    "            # Default to random\n",
    "            selected_scan = subject_data.sample(n=1, random_state=np.random.randint(0, 10000)).iloc[0]\n",
    "        \n",
    "        selected_scans.append(selected_scan)\n",
    "    \n",
    "    # Create DataFrame from selected scans\n",
    "    selected_df = pd.DataFrame(selected_scans)\n",
    "    \n",
    "    print(f\"‚úÖ Selected {len(selected_df)} scans (1 per subject)\")\n",
    "    \n",
    "    # Show selection summary\n",
    "    print(f\"\\nüìä SELECTION SUMMARY:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Original scans: {len(df)}\")\n",
    "    print(f\"Selected scans: {len(selected_df)}\")\n",
    "    print(f\"Reduction: {len(df) - len(selected_df)} scans removed\")\n",
    "    \n",
    "    # Group distribution\n",
    "    group_dist = selected_df['Group'].value_counts()\n",
    "    print(f\"\\nSelected dataset distribution:\")\n",
    "    for group, count in group_dist.items():\n",
    "        print(f\"  {group}: {count} subjects\")\n",
    "    \n",
    "    return selected_df\n",
    "\n",
    "def copy_selected_scans(source_dir, target_dir, selected_df, resume=True):\n",
    "    \"\"\"\n",
    "    Copy selected scans to new directory maintaining structure\n",
    "    \n",
    "    Args:\n",
    "        source_dir: Source directory path\n",
    "        target_dir: Target directory path  \n",
    "        selected_df: DataFrame with selected scans\n",
    "        resume: Whether to skip already copied scans\n",
    "    \"\"\"\n",
    "    source_path = Path(source_dir)\n",
    "    target_path = Path(target_dir)\n",
    "    \n",
    "    # Create target directory\n",
    "    target_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"üìÅ Copying selected scans...\")\n",
    "    print(f\"   Source: {source_dir}\")\n",
    "    print(f\"   Target: {target_dir}\")\n",
    "    \n",
    "    copied_count = 0\n",
    "    skipped_count = 0\n",
    "    error_count = 0\n",
    "    \n",
    "    # Copy each selected scan\n",
    "    with tqdm(total=len(selected_df), desc=\"Copying scans\") as pbar:\n",
    "        for _, scan in selected_df.iterrows():\n",
    "            try:\n",
    "                # Parse scan path to get components\n",
    "                scan_path_parts = Path(scan['ScanPath']).parts\n",
    "                subject_id = scan_path_parts[0]\n",
    "                scan_type = scan_path_parts[1]\n",
    "                timestamp = scan_path_parts[2]\n",
    "                image_data_id = scan_path_parts[3]\n",
    "                \n",
    "                # Source and target paths\n",
    "                source_scan_dir = source_path / subject_id / scan_type / timestamp / image_data_id\n",
    "                target_scan_dir = target_path / subject_id / scan_type / timestamp / image_data_id\n",
    "                \n",
    "                # Check if already copied (resume functionality)\n",
    "                if resume and target_scan_dir.exists():\n",
    "                    # Check if target has same number of files\n",
    "                    if source_scan_dir.exists():\n",
    "                        source_files = list(source_scan_dir.glob(\"*.dcm\"))\n",
    "                        target_files = list(target_scan_dir.glob(\"*.dcm\"))\n",
    "                        if len(source_files) == len(target_files):\n",
    "                            skipped_count += 1\n",
    "                            pbar.set_postfix({\n",
    "                                'Copied': copied_count, \n",
    "                                'Skipped': skipped_count,\n",
    "                                'Errors': error_count\n",
    "                            })\n",
    "                            pbar.update(1)\n",
    "                            continue\n",
    "                \n",
    "                # Copy the scan directory\n",
    "                if source_scan_dir.exists():\n",
    "                    # Create parent directories\n",
    "                    target_scan_dir.parent.mkdir(parents=True, exist_ok=True)\n",
    "                    \n",
    "                    # Remove target if it exists (for clean copy)\n",
    "                    if target_scan_dir.exists():\n",
    "                        shutil.rmtree(target_scan_dir)\n",
    "                    \n",
    "                    # Copy the directory\n",
    "                    shutil.copytree(source_scan_dir, target_scan_dir)\n",
    "                    copied_count += 1\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è  Source not found: {source_scan_dir}\")\n",
    "                    error_count += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error copying {scan['Subject']}: {e}\")\n",
    "                error_count += 1\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'Copied': copied_count, \n",
    "                'Skipped': skipped_count,\n",
    "                'Errors': error_count\n",
    "            })\n",
    "            pbar.update(1)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Copy completed!\")\n",
    "    print(f\"   Copied: {copied_count} scans\")\n",
    "    print(f\"   Skipped: {skipped_count} scans (already exist)\")\n",
    "    print(f\"   Errors: {error_count} scans\")\n",
    "    \n",
    "    return copied_count, skipped_count, error_count\n",
    "\n",
    "def create_single_scan_dataset(source_csv, source_dir, target_dir, selection_strategy='random'):\n",
    "    \"\"\"\n",
    "    Main function to create dataset with one scan per subject\n",
    "    \n",
    "    Args:\n",
    "        source_csv: Path to source metadata CSV\n",
    "        source_dir: Source directory with scans\n",
    "        target_dir: Target directory for new dataset\n",
    "        selection_strategy: 'random', 'earliest', 'latest', or 'baseline'\n",
    "    \"\"\"\n",
    "    print(\"üöÄ Creating single-scan dataset...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Step 1: Select one scan per subject\n",
    "    selected_df = select_one_scan_per_subject(source_csv, selection_strategy)\n",
    "    \n",
    "    # Step 2: Copy selected scans\n",
    "    copied_count, skipped_count, error_count = copy_selected_scans(\n",
    "        source_dir, target_dir, selected_df, resume=True\n",
    "    )\n",
    "    \n",
    "    # Step 3: Save updated metadata\n",
    "    target_metadata_path = Path(target_dir) / \"metadata.csv\"\n",
    "    \n",
    "    # Remove the TimestampDateTime column before saving\n",
    "    metadata_to_save = selected_df.drop('TimestampDateTime', axis=1)\n",
    "    \n",
    "    metadata_to_save.to_csv(target_metadata_path, index=False)\n",
    "    print(f\"üíæ Saved metadata: {target_metadata_path}\")\n",
    "    \n",
    "    # Final summary\n",
    "    print(f\"\\nüéâ DATASET CREATION COMPLETE!\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Strategy used: {selection_strategy}\")\n",
    "    print(f\"Original dataset: {len(pd.read_csv(source_csv))} scans\")\n",
    "    print(f\"New dataset: {len(selected_df)} scans (1 per subject)\")\n",
    "    print(f\"Files copied: {copied_count}\")\n",
    "    print(f\"Target directory: {target_dir}\")\n",
    "    print(f\"Metadata file: {target_metadata_path}\")\n",
    "    \n",
    "    return selected_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    source_csv = r\"C:\\Users\\User\\github_repos\\AD_CN_train_v1\\metadata.csv\"\n",
    "    source_dir = r\"C:\\Users\\User\\github_repos\\AD_CN_train_v1\"\n",
    "    target_dir = r\"C:\\Users\\User\\github_repos\\AD_CN_train_v2\"\n",
    "    \n",
    "    # Create the dataset\n",
    "    selected_df = create_single_scan_dataset(\n",
    "        source_csv=source_csv,\n",
    "        source_dir=source_dir, \n",
    "        target_dir=target_dir,\n",
    "        selection_strategy='random'  # Options: 'random', 'earliest', 'latest', 'baseline'\n",
    "    )\n",
    "    \n",
    "    print(\"\\n‚úÖ Single-scan dataset creation completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c02c31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adni-xNbsbYGy-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
