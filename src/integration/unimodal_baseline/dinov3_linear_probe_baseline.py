"""
Evaluate sMRI-only baseline using DINOv3 backbone + sklearn logistic regression
on the SAME cross-validation splits as fusion/protein baselines.

This script performs INFERENCE ONLY per fold (no training):
1) Use DINOv3's slice-wise aggregation preprocessing for 3D MRI volumes
2) Read CV splits generated by fusion (expects 'test' indices per fold)
3) Load pretrained DINOv3 backbone
4) Load trained sklearn logistic regression model (.pkl from joblib)
5) Extract features -> logistic regression -> predictions on test set per fold
6) Compute the same metrics as protein baseline (Accuracy, Balanced Acc, AUC, Precision, Recall, F1, MCC)
7) Aggregate metrics across folds and save JSON

Key differences from BrainIAC baseline:
- Uses DINOv3's slice-wise aggregation for 3D->2D feature extraction
- Uses sklearn LogisticRegression instead of PyTorch linear probe
- Requires joblib-saved .pkl model checkpoint
- Leverages DINOv3's MONAI-based preprocessing pipeline (identical to linear probe experiments)
"""

import argparse
import json
from pathlib import Path
from typing import List, Tuple
import sys
import tempfile

import numpy as np
import pandas as pd

import torch
import torch.nn as nn

from sklearn.metrics import (
    accuracy_score,
    balanced_accuracy_score,
    confusion_matrix,
    precision_recall_fscore_support,
    roc_auc_score,
    matthews_corrcoef,
)

# Add DINOv3 to path
_dinov3_root = Path(__file__).resolve().parents[2] / 'mri' / 'dinov3'
_dinov3_root_str = str(_dinov3_root)
if _dinov3_root_str not in sys.path:
    sys.path.insert(0, _dinov3_root_str)

# Import DINOv3 modules
from dinov3.data.transforms import make_classification_eval_transform
from dinov3.data.loaders import make_dataset
from dinov3.eval.utils import extract_features


def normalize_path(path_str: str) -> Path:
    """Normalize to absolute path, stripping duplicate project prefix if present."""
    if isinstance(path_str, str) and path_str.startswith('multimodal-AD/'):
        cwd = Path.cwd()
        if cwd.name == 'multimodal-AD' or str(cwd).endswith('/multimodal-AD'):
            path_str = path_str.replace('multimodal-AD/', '', 1)
    return Path(path_str).expanduser().resolve()


def create_temp_adni_csv(df_subset: pd.DataFrame, output_path: Path, mri_root: Path) -> None:
    """Convert multimodal CSV format to ADNI-compatible format.
    
    Args:
        df_subset: Subset of multimodal CSV with 'mri_path' and 'research_group' columns
        output_path: Where to save temporary ADNI-format CSV
        mri_root: Root directory containing images (for validation)
    """
    adni_data = []
    
    for _, row in df_subset.iterrows():
        # Extract pat_id from mri_path filename
        # e.g., /path/to/126_S_0606.nii.gz -> 126_S_0606
        mri_path = Path(row['mri_path'])
        pat_id = mri_path.stem.replace('.nii', '')  # Remove .nii if present (.nii.gz -> .nii -> '')
        
        # Convert research_group to binary label
        label = 1 if row['research_group'] == 'AD' else 0
        
        adni_data.append({
            'pat_id': pat_id,
            'label': label,
        })
    
    # Save to CSV
    output_path.parent.mkdir(parents=True, exist_ok=True)
    pd.DataFrame(adni_data).to_csv(output_path, index=False)
    
    print(f"  Created temp ADNI CSV: {output_path}")
    print(f"    {len(adni_data)} samples: {sum(d['label'] for d in adni_data)} AD, {len(adni_data) - sum(d['label'] for d in adni_data)} CN")

def load_dinov3_model(hub_repo_dir: Path, hub_model: str, pretrained_weights: Path, device: torch.device) -> nn.Module:
    """
    Load pretrained DINOv3 model using torch.hub.
    
    Args:
        hub_repo_dir: Path to DINOv3 repository
        hub_model: Model name (e.g., 'dinov3_vits16', 'dinov3_vitb16')
        pretrained_weights: Path to .pth weights file
        device: Device to load model on
    
    Returns:
        DINOv3 model in eval mode
    """
    print(f"Loading DINOv3 model: {hub_model}")
    print(f"  Hub repo: {hub_repo_dir}")
    print(f"  Weights: {pretrained_weights}")
    
    # Load model using torch.hub (local)
    model = torch.hub.load(
        str(hub_repo_dir),
        hub_model,
        source='local',
        pretrained=False
    )
    
    # Load pretrained weights
    state_dict = torch.load(str(pretrained_weights), map_location='cpu')
    model.load_state_dict(state_dict, strict=True)
    
    model.eval()
    model.to(device)
    
    print(f"  Model loaded successfully")
    return model


def load_sklearn_logreg(checkpoint_path: Path):
    """
    Load sklearn LogisticRegression model from joblib checkpoint.
    
    Args:
        checkpoint_path: Path to .pkl file saved by joblib.dump()
    
    Returns:
        Trained sklearn LogisticRegression estimator
    """
    import joblib
    
    if not checkpoint_path.exists():
        raise FileNotFoundError(f"Checkpoint not found: {checkpoint_path}")
    
    model = joblib.load(str(checkpoint_path))
    print(f"Loaded sklearn LogisticRegression from {checkpoint_path}")
    return model


def _safe_auc_score(y_true: np.ndarray, y_prob: np.ndarray) -> float:
    """Return AUC with robust handling for degenerate cases."""
    unique_labels = np.unique(y_true)
    unique_preds = np.unique((y_prob >= 0.5).astype(int))
    if len(unique_labels) < 2:
        return float('nan')
    if len(unique_preds) < 2:
        return 0.5
    try:
        return float(roc_auc_score(y_true, y_prob))
    except ValueError:
        return float('nan')


def evaluate_fold(
    dinov3_model: nn.Module,
    logreg_model,
    device: torch.device,
    df: pd.DataFrame,
    test_indices: np.ndarray,
    mri_root: Path,
    temp_csv_dir: Path,
    fold_idx: int,
    image_size: int = 224,
    slice_axis: int = 0,
    stride: int = 2,
) -> dict:
    """
    Run inference for a single fold's test set and compute metrics.
    
    Uses DINOv3's dataset infrastructure (make_dataset + SliceAggregationDataset)
    to ensure identical preprocessing as linear probe experiments.
    
    Args:
        dinov3_model: Pretrained DINOv3 backbone
        logreg_model: Trained sklearn LogisticRegression
        device: Device for DINOv3 inference
        df: Full dataframe with all samples
        test_indices: Indices of test samples for this fold
        mri_root: Root directory containing NIfTI images
        temp_csv_dir: Directory for temporary ADNI-format CSVs
        fold_idx: Fold index (for temp CSV naming)
        image_size: Image size for DINOv3 transforms
        slice_axis: Axis to slice along (0=sagittal, 1=coronal, 2=axial)
        stride: Slice stride for aggregation
    
    Returns:
        Dict with test metrics for this fold
    """
    # Get test subset
    df_test = df.iloc[test_indices].reset_index(drop=True)

    # Validate required columns
    if 'mri_path' not in df_test.columns or 'research_group' not in df_test.columns:
        raise KeyError("CSV must contain 'mri_path' and 'research_group' columns for DINOv3 baseline")

    # Create temporary ADNI-format CSV
    temp_csv_path = temp_csv_dir / f"dinov3_baseline_fold{fold_idx}_test.csv"
    create_temp_adni_csv(df_test, temp_csv_path, mri_root)

    # Build dataset using DINOv3's make_dataset (automatically wraps with SliceAggregationDataset)
    transform = make_classification_eval_transform(
        resize_size=image_size,
        crop_size=image_size
    )
    
    dataset_str = f"ADNI:split=TEST:root={mri_root}:extra={temp_csv_dir}:csv_filename={temp_csv_path.name}"
    print(f"  Building dataset with string: {dataset_str}")
    dataset = make_dataset(
        dataset_str=dataset_str,
        transform=transform,
    )
    
    print(f"  Created DINOv3 dataset: {len(dataset)} samples")
        
    # Extract features using DINOv3's extract_features (routes to slice aggregation automatically)
    features, labels_tensor = extract_features(
        model=dinov3_model,
        dataset=dataset,
        batch_size=1,
        num_workers=0,
        gather_on_cpu=True  # Ensure features are on CPU for sklearn
    )

    # Convert to numpy
    features_np = features.cpu().numpy()  # [N, feature_dim]
    labels_np = labels_tensor.cpu().numpy()  # [N]

    print(f"  Extracted features: shape={features_np.shape}, dtype={features_np.dtype}")
    print(f"    Feature stats: min={features_np.min():.4f}, max={features_np.max():.4f}, mean={features_np.mean():.4f}, std={features_np.std():.4f}")
    print(f"    Labels: {len(labels_np)} samples, AD={int((labels_np == 1).sum())}, CN={int((labels_np == 0).sum())}")

    # Batch prediction: Get predictions for all test samples at once
    # features_np shape: [N, feature_dim] where N = number of test samples
    print(f"  Running sklearn batch inference on {len(features_np)} samples...")
    probas = logreg_model.predict_proba(features_np)  # [N, 2] - all samples in one batch
    y_prob = probas[:, 1]  # Probability of AD (class 1) for each sample
    y_pred = logreg_model.predict(features_np)  # [N] - all predictions in one batch
    y_true = labels_np.astype(int)

    # Show prediction statistics
    print(f"  Predictions summary:")
    print(f"    Probabilities: min={y_prob.min():.4f}, max={y_prob.max():.4f}, mean={y_prob.mean():.4f}")
    print(f"    Predicted classes: AD={int((y_pred == 1).sum())}, CN={int((y_pred == 0).sum())}")
    

    # Compute metrics
    test_acc = float(accuracy_score(y_true, y_pred))
    test_balanced_acc = float(balanced_accuracy_score(y_true, y_pred))
    precision, recall, f1, _ = precision_recall_fscore_support(
        y_true, y_pred, average='binary', zero_division=0
    )
    test_auc = _safe_auc_score(y_true, y_prob)
    test_cm = confusion_matrix(y_true, y_pred, labels=[0, 1])
    test_mcc = float(matthews_corrcoef(y_true, y_pred))

    # Clean up temp CSV
    temp_csv_path.unlink(missing_ok=True)

    return {
        'test_acc': test_acc,
        'test_balanced_acc': test_balanced_acc,
        'test_auc': test_auc,
        'test_precision': float(precision),
        'test_recall': float(recall),
        'test_f1': float(f1),
        'test_mcc': test_mcc,
        'test_cm': test_cm.tolist(),
        'n_test': int(len(y_true)),
    }


def print_fold_results(results: dict, fold_idx: int) -> None:
    """Print fold results in a formatted way."""
    print(f"\nFold {fold_idx} Test Results:")
    print(f"  Accuracy: {results['test_acc']:.4f}")
    print(f"  Balanced Accuracy: {results['test_balanced_acc']:.4f}")
    if np.isnan(results['test_auc']):
        print("  AUC: undefined")
    else:
        print(f"  AUC: {results['test_auc']:.4f}")
    print(f"  Precision: {results['test_precision']:.4f}")
    print(f"  Recall: {results['test_recall']:.4f}")
    print(f"  F1: {results['test_f1']:.4f}")
    print(f"  MCC: {results['test_mcc']:.4f}")
    cm = np.array(results['test_cm'])
    print("  Confusion Matrix:")
    print(f"    TN={cm[0,0]}, FP={cm[0,1]}")
    print(f"    FN={cm[1,0]}, TP={cm[1,1]}")


def aggregate_results(fold_results: List[dict]) -> Tuple[dict, np.ndarray]:
    """Aggregate results across folds."""
    metrics = ['test_acc', 'test_balanced_acc', 'test_auc', 'test_precision', 'test_recall', 'test_f1', 'test_mcc']
    aggregated = {}

    print("\n" + "="*60)
    print("CROSS-VALIDATION RESULTS SUMMARY (DINOv3 sMRI-ONLY BASELINE)")
    print("="*60)

    for metric in metrics:
        values = [fr[metric] for fr in fold_results]
        if metric == 'test_auc' and any(np.isnan(v) for v in values):
            aggregated[metric] = {
                'mean': float('nan'),
                'std': float('nan'),
                'values': values,
            }
            name = metric.replace('test_', '').replace('_', ' ').upper()
            print(f"{name}: undefined (some folds had undefined AUC)")
        else:
            valid = [v for v in values if not (isinstance(v, float) and np.isnan(v))]
            if valid:
                aggregated[metric] = {
                    'mean': float(np.mean(valid)),
                    'std': float(np.std(valid)),
                    'values': values,
                }
                name = metric.replace('test_', '').replace('_', ' ').upper()
                print(f"{name}: {aggregated[metric]['mean']:.4f} ± {aggregated[metric]['std']:.4f}")
            else:
                aggregated[metric] = {
                    'mean': float('nan'),
                    'std': float('nan'),
                    'values': values,
                }
                name = metric.replace('test_', '').replace('_', ' ').upper()
                print(f"{name}: undefined (all folds had undefined values)")

    total_cm = np.sum([np.array(fr['test_cm']) for fr in fold_results], axis=0)
    print("\nAggregated Confusion Matrix:")
    print(f"  TN={total_cm[0,0]}, FP={total_cm[0,1]}")
    print(f"  FN={total_cm[1,0]}, TP={total_cm[1,1]}")

    return aggregated, total_cm


def main():
    parser = argparse.ArgumentParser(
        description="Evaluate DINOv3 linear-probe sMRI model on fusion CV splits (inference only)"
    )
    parser.add_argument('--data-csv', type=str, required=True,
                        help='CSV containing paired dataset with columns: pat_id, label, mri_path, research_group')
    parser.add_argument('--cv-splits-json', type=str, required=True,
                        help='cv_splits.json produced by fusion run (expects list of {test: [...]})')
    parser.add_argument('--checkpoint-path', type=str, required=True,
                        help='Path to sklearn LogisticRegression checkpoint (.pkl from joblib)')
    parser.add_argument('--dinov3-hub-dir', type=str, required=True,
                        help='Path to DINOv3 repository directory')
    parser.add_argument('--dinov3-model', type=str, default='dinov3_vits16',
                        choices=['dinov3_vits16', 'dinov3_vitb16', 'dinov3_vitl16'],
                        help='DINOv3 model variant')
    parser.add_argument('--pretrained-weights', type=str, required=True,
                        help='Path to DINOv3 pretrained weights (.pth)')
    parser.add_argument('--mri-root', type=str, required=True,
                        help='Root directory containing NIfTI images (pat_id.nii.gz)')
    parser.add_argument('--save-dir', type=str, default=None,
                        help='Directory to save results JSON')
    parser.add_argument('--device', type=str, default='cuda' if torch.cuda.is_available() else 'cpu',
                        choices=['cpu', 'cuda'])
    parser.add_argument('--image-size', type=int, default=224,
                        help='Image size for DINOv3 transforms')
    parser.add_argument('--slice-axis', type=int, default=0,
                        help='Axis to slice along (0=sagittal, 1=coronal, 2=axial)')
    parser.add_argument('--stride', type=int, default=2,
                        help='Slice stride for aggregation')
    args = parser.parse_args()

    # Normalize paths
    data_csv = normalize_path(args.data_csv)
    cv_splits_json = normalize_path(args.cv_splits_json)
    checkpoint_path = normalize_path(args.checkpoint_path)
    dinov3_hub_dir = normalize_path(args.dinov3_hub_dir)
    pretrained_weights = normalize_path(args.pretrained_weights)
    mri_root = normalize_path(args.mri_root)
    save_dir = normalize_path(args.save_dir) if args.save_dir else None

    device = torch.device(args.device)

    print("="*60)
    print("DINOv3 sMRI-ONLY BASELINE EVALUATION")
    print("="*60)
    print(f"Data CSV: {data_csv}")
    print(f"CV Splits: {cv_splits_json}")
    print(f"Checkpoint: {checkpoint_path}")
    print(f"DINOv3 Hub: {dinov3_hub_dir}")
    print(f"DINOv3 Model: {args.dinov3_model}")
    print(f"Pretrained Weights: {pretrained_weights}")
    print(f"MRI Root: {mri_root}")
    print(f"Device: {device}")
    print(f"Image Size: {args.image_size}")
    print(f"Slice Axis: {args.slice_axis}, Stride: {args.stride}")
    print()

    # Load data and splits
    df = pd.read_csv(str(data_csv))
    with open(str(cv_splits_json), 'r') as f:
        cv_splits = json.load(f)

    print(f"Loaded {len(df)} samples from CSV")
    print(f"Loaded {len(cv_splits)} CV folds")
    print()

    # Load DINOv3 model (frozen, inference only)
    dinov3_model = load_dinov3_model(
        hub_repo_dir=dinov3_hub_dir,
        hub_model=args.dinov3_model,
        pretrained_weights=pretrained_weights,
        device=device
    )

    # Load sklearn LogisticRegression
    logreg_model = load_sklearn_logreg(checkpoint_path)
    print()

    # Create temporary directory for ADNI-format CSVs
    temp_csv_dir = Path(tempfile.mkdtemp(prefix="dinov3_baseline_"))
    print(f"Using temp directory for CSVs: {temp_csv_dir}")
    print()

    try:
        fold_results = []
        for fold_idx, split in enumerate(cv_splits, start=1):
            # Expect protein/fusion-style splits with 'test' indices
            if 'test' not in split:
                raise KeyError("CV split must contain 'test' key with indices to evaluate")
            test_idx = np.asarray(split['test'], dtype=int)

            print("-"*60)
            print(f"FOLD {fold_idx}/{len(cv_splits)}  |  Test size: {len(test_idx)}")

            res = evaluate_fold(
                dinov3_model=dinov3_model,
                logreg_model=logreg_model,
                device=device,
                df=df,
                test_indices=test_idx,
                mri_root=mri_root,
                temp_csv_dir=temp_csv_dir,
                fold_idx=fold_idx,
                image_size=args.image_size,
                slice_axis=args.slice_axis,
                stride=args.stride,
            )
            res['fold'] = fold_idx
            fold_results.append(res)
            print_fold_results(res, fold_idx)
    finally:
        # Clean up temp directory
        import shutil
        if temp_csv_dir.exists():
            shutil.rmtree(temp_csv_dir)
            print(f"\nCleaned up temp directory: {temp_csv_dir}")

    aggregated, total_cm = aggregate_results(fold_results)

    if save_dir is not None:
        save_dir.mkdir(parents=True, exist_ok=True)
        output = {
            'fold_results': fold_results,
            'aggregated_metrics': {
                k: {
                    'mean': None if isinstance(v['mean'], float) and np.isnan(v['mean']) else v['mean'],
                    'std': None if isinstance(v['std'], float) and np.isnan(v['std']) else v['std'],
                    'values': [None if isinstance(x, float) and np.isnan(x) else x for x in v['values']],
                } for k, v in aggregated.items()
            },
            'aggregated_cm': total_cm.tolist(),
        }
        with open(save_dir / 'dinov3_linear_probe_baseline_results.json', 'w') as f:
            json.dump(output, f, indent=2)
        print(f"\n✅ Results saved to: {save_dir / 'dinov3_linear_probe_baseline_results.json'}")


if __name__ == '__main__':
    main()

