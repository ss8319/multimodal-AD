{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ§ ðŸ”¬ Cross-Attention Transformer for MRI + Proteomic Fusion\n",
        "\n",
        "## A Complete Walkthrough: From Theory to Implementation\n",
        "\n",
        "This notebook demonstrates how to build a **cross-attention transformer** that fuses MRI spatial features with proteomic biomarkers for Alzheimer's Disease classification.\n",
        "\n",
        "### ðŸ“‹ **What You'll Learn:**\n",
        "1. **PyTorch vs Custom Implementation** - When to use each\n",
        "2. **Positional Encoding** - Spatial (MRI) vs Categorical (Proteins)\n",
        "3. **Cross-Attention Mechanism** - How modalities \"talk\" to each other\n",
        "4. **Attention Visualization** - Understanding model decisions\n",
        "5. **End-to-End Training** - From data to predictions\n",
        "\n",
        "### ðŸŽ¯ **Architecture Overview:**\n",
        "```\n",
        "MRI Patches [B,100,768] â”€â”€â”\n",
        "                          â”‚\n",
        "                          â”œâ”€â”€ Cross-Attention â”€â”€> Classification\n",
        "                          â”‚\n",
        "Proteins [B,8,8] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“¦ **Setup & Imports**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n",
            "PyTorch version: 2.8.0+cpu\n"
          ]
        }
      ],
      "source": [
        "# Install required packages (if needed)\n",
        "# !pip install torch torchvision matplotlib seaborn scikit-learn\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import math\n",
        "from typing import Dict, Tuple, Optional, List\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Device setup\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ¤” **Design Decisions: PyTorch Built-in vs Custom Implementation**\n",
        "\n",
        "### âœ… **Use PyTorch Built-in:**\n",
        "- **`nn.MultiheadAttention`** - Mature, optimized, well-tested\n",
        "- **`nn.TransformerEncoder/TransformerEncoderLayer`** - For self-attention blocks\n",
        "- **`nn.Embedding`** - For categorical positional encoding\n",
        "- **`nn.Linear`, `nn.LayerNorm`, `nn.GELU`** - Standard components\n",
        "\n",
        "### ðŸ”§ **Custom Implementation:**\n",
        "- **Cross-attention orchestration** - PyTorch doesn't have cross-modal transformers\n",
        "- **3D spatial positional encoding** - Domain-specific for brain imaging\n",
        "- **Multimodal fusion strategy** - Research-specific architecture\n",
        "- **Attention visualization tools** - For interpretability\n",
        "\n",
        "### ðŸŽ¯ **Best of Both Worlds:**\n",
        "This approach gives us **reliability** (PyTorch) + **flexibility** (custom) + **maintainability** (less code to debug)!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸŽ¯ **Step 1: Positional Encoding - The Foundation**\n",
        "\n",
        "Positional encoding is **critical** for both modalities:\n",
        "- **MRI**: Spatial brain anatomy (hippocampus vs cortex)\n",
        "- **Proteins**: Biological function categories (amyloid vs inflammation)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defined 8 protein categories for AD research\n",
            "Categories: ['amyloid_pathway', 'tau_pathway', 'inflammation', 'neurodegeneration', 'synaptic', 'vascular', 'metabolic', 'other']\n"
          ]
        }
      ],
      "source": [
        "class Spatial3DPositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    3D spatial positional encoding for MRI brain patches.\n",
        "    Encodes actual anatomical coordinates in 3D brain space.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, embed_dim: int, max_patches: int = 1000):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        \n",
        "        # Project 3D coordinates (x,y,z) to embedding dimension\n",
        "        self.coord_projection = nn.Sequential(\n",
        "            nn.Linear(3, embed_dim // 2),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(embed_dim // 2, embed_dim)\n",
        "        )\n",
        "        \n",
        "        # Default grid coordinates for patches (if real coordinates unavailable)\n",
        "        self.register_buffer('default_coords', self._create_default_grid(max_patches))\n",
        "    \n",
        "    def _create_default_grid(self, max_patches: int) -> torch.Tensor:\n",
        "        \"\"\"Create a default 3D grid of coordinates.\"\"\"\n",
        "        # Assume cubic grid: find cube root\n",
        "        grid_size = int(np.ceil(max_patches ** (1/3)))\n",
        "        \n",
        "        # Create 3D coordinate grid\n",
        "        coords = torch.linspace(-1, 1, grid_size)\n",
        "        xx, yy, zz = torch.meshgrid(coords, coords, coords, indexing='ij')\n",
        "        \n",
        "        # Flatten and take first max_patches coordinates\n",
        "        grid_coords = torch.stack([xx.flatten(), yy.flatten(), zz.flatten()], dim=1)\n",
        "        return grid_coords[:max_patches]\n",
        "    \n",
        "    def forward(self, x: torch.Tensor, coordinates: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: [batch_size, num_patches, embed_dim] - MRI patch embeddings\n",
        "            coordinates: [batch_size, num_patches, 3] - 3D spatial coordinates (optional)\n",
        "        \"\"\"\n",
        "        batch_size, num_patches, _ = x.shape\n",
        "        \n",
        "        if coordinates is not None:\n",
        "            coords = coordinates\n",
        "        else:\n",
        "            # Use default grid coordinates\n",
        "            coords = self.default_coords[:num_patches].unsqueeze(0).repeat(batch_size, 1, 1)\n",
        "        \n",
        "        # Project coordinates to embedding space\n",
        "        pos_encoding = self.coord_projection(coords)\n",
        "        \n",
        "        return x + pos_encoding\n",
        "\n",
        "\n",
        "class CategoricalPositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    Categorical positional encoding for proteomic features.\n",
        "    Groups proteins by biological pathway/function using PyTorch Embedding.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, embed_dim: int, protein_categories: List[str]):\n",
        "        super().__init__()\n",
        "        self.protein_categories = protein_categories\n",
        "        self.num_categories = len(protein_categories)\n",
        "        \n",
        "        # Use PyTorch's optimized Embedding layers\n",
        "        self.category_embedding = nn.Embedding(self.num_categories, embed_dim)\n",
        "        self.position_embedding = nn.Embedding(50, embed_dim)  # Max 50 proteins per category\n",
        "        \n",
        "        # Initialize embeddings with small random values\n",
        "        nn.init.normal_(self.category_embedding.weight, std=0.02)\n",
        "        nn.init.normal_(self.position_embedding.weight, std=0.02)\n",
        "    \n",
        "    def forward(self, x: torch.Tensor, \n",
        "                category_ids: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: [batch_size, num_proteins, embed_dim]\n",
        "            category_ids: [num_proteins] - category ID for each protein\n",
        "        \"\"\"\n",
        "        batch_size, num_proteins, _ = x.shape\n",
        "        \n",
        "        if category_ids is None:\n",
        "            # Default: distribute proteins across categories\n",
        "            category_ids = torch.arange(num_proteins, device=x.device) % self.num_categories\n",
        "        \n",
        "        # Position within category\n",
        "        position_ids = torch.arange(num_proteins, device=x.device) // self.num_categories\n",
        "        \n",
        "        # Get embeddings\n",
        "        category_emb = self.category_embedding(category_ids)  # [num_proteins, embed_dim]\n",
        "        position_emb = self.position_embedding(position_ids)  # [num_proteins, embed_dim]\n",
        "        \n",
        "        # Combine and add to input\n",
        "        pos_encoding = category_emb + position_emb\n",
        "        return x + pos_encoding.unsqueeze(0)  # Broadcast across batch\n",
        "\n",
        "\n",
        "# Define protein categories for AD research\n",
        "AD_PROTEIN_CATEGORIES = [\n",
        "    'amyloid_pathway',    # AÎ²40, AÎ²42\n",
        "    'tau_pathway',        # p-tau, t-tau\n",
        "    'inflammation',       # cytokines\n",
        "    'neurodegeneration',  # neurofilament\n",
        "    'synaptic',          # synaptic proteins\n",
        "    'vascular',          # vascular markers\n",
        "    'metabolic',         # metabolic proteins\n",
        "    'other'              # uncategorized/\n",
        "]\n",
        "\n",
        "print(f\"Defined {len(AD_PROTEIN_CATEGORIES)} protein categories for AD research\")\n",
        "print(\"Categories:\", AD_PROTEIN_CATEGORIES)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”„ **Step 2: Cross-Attention Transformer (PyTorch + Custom)**\n",
        "\n",
        "Here we use **PyTorch's built-in** components where possible, but implement **custom cross-modal orchestration**.\n",
        "\n",
        "### ðŸ—ï¸ **Architecture Strategy:**\n",
        "1. **Self-attention**: Use `nn.TransformerEncoderLayer` (PyTorch built-in)\n",
        "2. **Cross-attention**: Use `nn.MultiheadAttention` (PyTorch built-in)\n",
        "3. **Orchestration**: Custom logic to coordinate cross-modal attention\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Cross-attention transformer implemented using PyTorch + custom components!\n"
          ]
        }
      ],
      "source": [
        "class MultimodalCrossAttentionTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Complete multimodal transformer using PyTorch components + custom cross-modal logic.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, \n",
        "                 mri_embed_dim: int = 768,\n",
        "                 proteomic_embed_dim: int = 8,\n",
        "                 hidden_dim: int = 256,\n",
        "                 num_heads: int = 8,\n",
        "                 num_self_layers: int = 2,\n",
        "                 num_cross_layers: int = 2,\n",
        "                 num_classes: int = 2,\n",
        "                 dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        # Input projections to common hidden dimension\n",
        "        self.mri_projection = nn.Linear(mri_embed_dim, hidden_dim)\n",
        "        self.proteomic_projection = nn.Linear(proteomic_embed_dim, hidden_dim)\n",
        "        \n",
        "        # Positional encodings\n",
        "        self.mri_pos_encoder = Spatial3DPositionalEncoding(hidden_dim)\n",
        "        self.protein_pos_encoder = CategoricalPositionalEncoding(hidden_dim, AD_PROTEIN_CATEGORIES)\n",
        "        \n",
        "        # Self-attention layers using PyTorch TransformerEncoderLayer\n",
        "        mri_encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=hidden_dim,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=hidden_dim * 4,\n",
        "            dropout=dropout,\n",
        "            activation='gelu',\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.mri_self_encoder = nn.TransformerEncoder(mri_encoder_layer, num_self_layers)\n",
        "        \n",
        "        protein_encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=hidden_dim,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=hidden_dim * 4,\n",
        "            dropout=dropout,\n",
        "            activation='gelu',\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.protein_self_encoder = nn.TransformerEncoder(protein_encoder_layer, num_self_layers)\n",
        "        \n",
        "        # Cross-attention layers using PyTorch MultiheadAttention\n",
        "        self.mri_to_protein_attention = nn.ModuleList([\n",
        "            nn.MultiheadAttention(hidden_dim, num_heads, dropout, batch_first=True)\n",
        "            for _ in range(num_cross_layers)\n",
        "        ])\n",
        "        \n",
        "        self.protein_to_mri_attention = nn.ModuleList([\n",
        "            nn.MultiheadAttention(hidden_dim, num_heads, dropout, batch_first=True)\n",
        "            for _ in range(num_cross_layers)\n",
        "        ])\n",
        "        \n",
        "        # Layer norms for residual connections\n",
        "        self.mri_cross_norms = nn.ModuleList([nn.LayerNorm(hidden_dim) for _ in range(num_cross_layers)])\n",
        "        self.protein_cross_norms = nn.ModuleList([nn.LayerNorm(hidden_dim) for _ in range(num_cross_layers)])\n",
        "        \n",
        "        # Classification head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim // 2, num_classes)\n",
        "        )\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, mri_embeddings: torch.Tensor, proteomic_embeddings: torch.Tensor,\n",
        "                mri_coordinates: Optional[torch.Tensor] = None,\n",
        "                protein_categories: Optional[torch.Tensor] = None,\n",
        "                return_attention: bool = False) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Forward pass through the multimodal transformer.\n",
        "        \n",
        "        Args:\n",
        "            mri_embeddings: [batch_size, num_patches, mri_embed_dim]\n",
        "            proteomic_embeddings: [batch_size, num_proteins, proteomic_embed_dim]\n",
        "            mri_coordinates: [batch_size, num_patches, 3] - optional 3D coordinates\n",
        "            protein_categories: [num_proteins] - optional category assignments\n",
        "            return_attention: whether to return attention weights for visualization\n",
        "        \"\"\"\n",
        "        # Project to common hidden dimension\n",
        "        mri_hidden = self.mri_projection(mri_embeddings)\n",
        "        protein_hidden = self.proteomic_projection(proteomic_embeddings)\n",
        "        \n",
        "        # Add positional encodings\n",
        "        mri_hidden = self.mri_pos_encoder(mri_hidden, mri_coordinates)\n",
        "        protein_hidden = self.protein_pos_encoder(protein_hidden, protein_categories)\n",
        "        \n",
        "        # Apply dropout\n",
        "        mri_hidden = self.dropout(mri_hidden)\n",
        "        protein_hidden = self.dropout(protein_hidden)\n",
        "        \n",
        "        # Self-attention within each modality (PyTorch built-in)\n",
        "        mri_hidden = self.mri_self_encoder(mri_hidden)\n",
        "        protein_hidden = self.protein_self_encoder(protein_hidden)\n",
        "        \n",
        "        # Cross-attention between modalities (custom orchestration with PyTorch attention)\n",
        "        attention_weights = {}\n",
        "        \n",
        "        for i, (mri_cross_attn, protein_cross_attn, mri_norm, protein_norm) in enumerate(\n",
        "            zip(self.mri_to_protein_attention, self.protein_to_mri_attention, \n",
        "                self.mri_cross_norms, self.protein_cross_norms)\n",
        "        ):\n",
        "            # MRI attending to proteins\n",
        "            mri_attended, mri_attn = mri_cross_attn(\n",
        "                query=mri_hidden, key=protein_hidden, value=protein_hidden,\n",
        "                need_weights=return_attention\n",
        "            )\n",
        "            mri_hidden = mri_norm(mri_hidden + mri_attended)\n",
        "            \n",
        "            # Proteins attending to MRI\n",
        "            protein_attended, protein_attn = protein_cross_attn(\n",
        "                query=protein_hidden, key=mri_hidden, value=mri_hidden,\n",
        "                need_weights=return_attention\n",
        "            )\n",
        "            protein_hidden = protein_norm(protein_hidden + protein_attended)\n",
        "            \n",
        "            if return_attention:\n",
        "                attention_weights[f'mri_to_protein_layer_{i}'] = mri_attn\n",
        "                attention_weights[f'protein_to_mri_layer_{i}'] = protein_attn\n",
        "        \n",
        "        # Global pooling and fusion\n",
        "        mri_pooled = torch.mean(mri_hidden, dim=1)  # [batch, hidden_dim]\n",
        "        protein_pooled = torch.mean(protein_hidden, dim=1)  # [batch, hidden_dim]\n",
        "        \n",
        "        # Concatenate modalities for classification\n",
        "        fused_features = torch.cat([mri_pooled, protein_pooled], dim=1)  # [batch, hidden_dim*2]\n",
        "        \n",
        "        # Classification\n",
        "        logits = self.classifier(fused_features)\n",
        "        \n",
        "        outputs = {'logits': logits}\n",
        "        if return_attention:\n",
        "            outputs['attention_weights'] = attention_weights\n",
        "            \n",
        "        return outputs\n",
        "\n",
        "\n",
        "print(\"âœ… Cross-attention transformer implemented using PyTorch + custom components!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ§ª **Step 3: Test the Complete Model**\n",
        "\n",
        "Let's test our hybrid PyTorch + custom implementation with sample data to see the cross-attention in action!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input shapes:\n",
            "  MRI embeddings: torch.Size([4, 64, 768])\n",
            "  Proteomic embeddings: torch.Size([4, 8, 8])\n",
            "  MRI coordinates: torch.Size([4, 64, 3])\n",
            "  Protein categories: torch.Size([8])\n",
            "\n",
            "Model outputs:\n",
            "  Logits shape: torch.Size([4, 2])\n",
            "  Number of attention maps: 4\n",
            "  Attention map keys: ['mri_to_protein_layer_0', 'protein_to_mri_layer_0', 'mri_to_protein_layer_1', 'protein_to_mri_layer_1']\n",
            "\n",
            "Model statistics:\n",
            "  Total parameters: 4,625,794\n",
            "  Trainable parameters: 4,625,794\n",
            "\n",
            "âœ… Model test successful!\n"
          ]
        }
      ],
      "source": [
        "# Create model\n",
        "model = MultimodalCrossAttentionTransformer(\n",
        "    mri_embed_dim=768,\n",
        "    proteomic_embed_dim=8,\n",
        "    hidden_dim=256,\n",
        "    num_heads=8,\n",
        "    num_cross_layers=2\n",
        ").to(device)\n",
        "\n",
        "# Create sample data\n",
        "batch_size = 4\n",
        "num_mri_patches = 64\n",
        "num_proteins = 8\n",
        "\n",
        "mri_embeddings = torch.randn(batch_size, num_mri_patches, 768).to(device)\n",
        "proteomic_embeddings = torch.randn(batch_size, num_proteins, 8).to(device)\n",
        "\n",
        "# Optional: provide spatial coordinates and protein categories\n",
        "mri_coordinates = torch.randn(batch_size, num_mri_patches, 3).to(device)\n",
        "protein_category_ids = torch.tensor([0, 0, 1, 1, 2, 3, 4, 5]).to(device)\n",
        "\n",
        "print(f\"Input shapes:\")\n",
        "print(f\"  MRI embeddings: {mri_embeddings.shape}\")\n",
        "print(f\"  Proteomic embeddings: {proteomic_embeddings.shape}\")\n",
        "print(f\"  MRI coordinates: {mri_coordinates.shape}\")\n",
        "print(f\"  Protein categories: {protein_category_ids.shape}\")\n",
        "\n",
        "# Forward pass\n",
        "with torch.no_grad():\n",
        "    outputs = model(\n",
        "        mri_embeddings=mri_embeddings,\n",
        "        proteomic_embeddings=proteomic_embeddings,\n",
        "        mri_coordinates=mri_coordinates,\n",
        "        protein_categories=protein_category_ids,\n",
        "        return_attention=True\n",
        "    )\n",
        "\n",
        "print(f\"\\nModel outputs:\")\n",
        "print(f\"  Logits shape: {outputs['logits'].shape}\")\n",
        "print(f\"  Number of attention maps: {len(outputs['attention_weights'])}\")\n",
        "print(f\"  Attention map keys: {list(outputs['attention_weights'].keys())}\")\n",
        "\n",
        "# Check model parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"\\nModel statistics:\")\n",
        "print(f\"  Total parameters: {total_params:,}\")\n",
        "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "print(\"\\nâœ… Model test successful!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "adni-xNbsbYGy-py3.12",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
